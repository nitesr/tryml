{"cells":[{"cell_type":"markdown","metadata":{"id":"UYwPYBDlced0"},"source":["In this assignment, we will use\n","* causality return\n","* value baseline\n","\n","to make policy gradient update in the Lunar lander environment.\n","\n","You can modify from the trainer implementation in [class demo](https://colab.research.google.com/drive/1tClkELFzT9WH7GFs-98VXKupmS149qIf#scrollTo=sQfNraq6e1ZR).\n","* Complete the return and advantage implementation.\n","* Update the trainer to call appropriate return/advantage computation for policy gradient updates.\n","* Implement the critic network, its loss function and back propagation logic.\n","\n","Reinforcement learning is sensitive to hyperparameters. Lunar Lander environment is considered solved with 200 episode return. Our simple implementations should generally be able to reach > 100 returns. Test hyperparameters such as learning rate, max episode length, number of epochs, number of steps per epoch, return/advantage computation to see what tweeks help.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"8fqL1z7xOgkO"},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: no matches found: gymnasium[box2d]\n","zsh:1: no matches found: gymnasium[classic-control]\n"]}],"source":["#@title Install dependencies for LunarLander.\n","!apt-get install swig > /dev/null 2>&1\n","!pip3 install numpy > /dev/null 2>&1\n","!pip3 install pandas > /dev/null 2>&1\n","!pip3 install matplotlib > /dev/null 2>&1\n","!pip3 install seaborn > /dev/null 2>&1 \n","!pip3 install gymnasium > /dev/null 2>&1\n","!pip3 install gymnasium[box2d] > /dev/null 2>&1\n","!pip3 install gymnasium[classic-control] > /dev/null 2>&1\n","!pip3 install pygame > /dev/null 2>&1\n","!pip3 install gym-box2d > /dev/null 2>&1"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","id":"fGCUizxlOSbc"},"outputs":[],"source":["#@title Import required libraries.\n","\n","import pandas as pd\n","import random\n","import seaborn as sns\n","\n","from IPython import display\n","import matplotlib.pyplot as plt\n","\n","from gymnasium.spaces import Discrete, Box"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import nn\n","from torch import optim\n","from torch.distributions import Categorical\n","\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","import gymnasium as gym\n","\n","\n","class ActorCriticNN(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, n_actions) -> None:\n","        super(ActorCriticNN, self).__init__()\n","        \n","        layers = []\n","        sizes = [input_size, *hidden_sizes]\n","        for j in range(len(sizes)-1):\n","            layers += [nn.Linear(sizes[j], sizes[j+1]), nn.ReLU()]\n","        \n","        self.backend = nn.Sequential(*layers)\n","        self.action_layer = nn.Linear(in_features=sizes[-1], out_features=n_actions)\n","        self.value_layer = nn.Linear(in_features=sizes[-1], out_features=1)\n","        \n","    def forward(self, x):\n","        x = self.backend(x)\n","        actions = self.action_layer(x)\n","        values = self.value_layer(x)\n","        return actions, values\n","    \n","\n","class ReinforceNN(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, n_actions) -> None:\n","        super(ReinforceNN, self).__init__()\n","        \n","        layers = []\n","        sizes = [input_size, *hidden_sizes, n_actions]\n","        for j in range(len(sizes)-1):\n","            act = nn.ReLU if j < len(sizes)-2 else nn.Identity\n","            layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n","        \n","        self.model = nn.Sequential(*layers)\n","        \n","    def forward(self, x):\n","        actions = self.model(x)\n","        return actions, None"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_DiJKqdsSf1H"},"outputs":[],"source":["# Trains a policy using policy gradient update.\n","class PolicyGradientTrainer():\n","    def __init__(self, env_name: str = 'LunarLander-v2', lr: float = 1e-2,\n","                 value_lr: float = 1e-3, max_eps_length: int = 500,\n","                 n_trajectories_per_epoch: int = 5000,\n","                 hidden_sizes: list[int] = [32], method: str = 'reinforce',\n","                 verbose: bool = False):\n","        \n","        if method not in {'reinforce', 'causality', 'advantage'}:\n","            raise ValueError('Unsupported method.')\n","        \n","        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n","        obs_dim = self.env.observation_space.shape[0]\n","        n_acts = self.env.action_space.n\n","        \n","        if method == 'advantage':\n","            self.model = ActorCriticNN(obs_dim, hidden_sizes, n_acts)\n","        else:\n","            self.model = ReinforceNN(obs_dim, hidden_sizes, n_acts)\n","        \n","        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","        self.max_eps_length = max_eps_length\n","        self.n_trajectories_per_epoch = n_trajectories_per_epoch\n","        self.verbose = verbose\n","    \n","    \n","    def sample_step(self, obs):\n","        logits, value = self.model(torch.as_tensor(obs, dtype=torch.float32))\n","        action = Categorical(logits=logits).sample().item()\n","        obs_prime, rew, done, _, _ = self.env.step(action)\n","        \n","        return action, rew, done, value.item(), obs_prime  \n","        \n","    def sample_trajectory(self, n=500):\n","        obs, _ = self.env.reset()\n","        \n","        tj_obs = []\n","        tj_actions = []\n","        tj_rewards = []\n","        tj_values = []\n","        \n","        for i in range(0, min(n, self.max_eps_length)):\n","            action, reward, done, value, obs_prime = self.sample_step(obs)\n","            tj_obs.append(obs)\n","            tj_actions.append(action)\n","            tj_rewards.append(reward)\n","            tj_values.append(value)\n","            obs = obs_prime\n","            \n","            if done:\n","                break\n","        \n","        tj_returns = []\n","        rt = 0\n","        for r in reversed(tj_rewards):\n","            rt += r\n","            tj_returns.append(rt)\n","        \n","        next_tj_values = [ *tj_values[1:],  tj_values[-1] ]\n","        tj_adv = np.array(tj_rewards) + np.array(tj_values) - np.array(next_tj_values)\n","        \n","        tj_exp_values = np.array(tj_rewards) + np.array([ *tj_values[1:], 0 ])\n","        \n","        return (tj_obs, tj_actions, tj_rewards, tj_returns, tj_adv, tj_exp_values)\n","    \n","    def compute_loss(self, log_ps, values, A, exp_values):\n","        loss_action = -1 * (log_ps * A).mean()\n","        loss_value = F.mse_loss(values, exp_values, reduction='mean')\n","        \n","        return loss_action + loss_value\n","    \n","    \n","    def train_epoch(self, i_epoch):\n","\n","        epoch_obs = []\n","        epoch_returns = []\n","        epoch_rewards = []\n","        epoch_actions = []\n","        epoch_exp_values = []\n","        epoch_ajdvs = []\n","        \n","        epoch_tj_total_return = []\n","        for i in range(0, self.n_trajectories_per_epoch):\n","            tj_obs, tj_actions, tj_rewards, tj_returns, tj_adv, tj_exp_values = self.sample_trajectory(self.max_eps_length)\n","            epoch_obs.extend(tj_obs)\n","            epoch_returns.extend(tj_returns)\n","            epoch_rewards.extend(tj_rewards)\n","            epoch_tj_total_return.append(np.sum(tj_rewards))\n","            epoch_actions.extend(tj_actions)\n","            epoch_exp_values.extend(tj_exp_values)\n","            epoch_ajdvs.extend(tj_adv)\n","            \n","        if self.verbose:\n","            print(f'epoch={i_epoch} mean return', np.mean(epoch_tj_total_return))\n","        \n","        epoch_obs_tensor = torch.as_tensor(np.array(epoch_obs), dtype=torch.float32)\n","        epoch_actions_tensor = torch.as_tensor(np.array(epoch_actions), dtype=torch.int16)\n","        epoch_adjs_tensor = torch.as_tensor(np.array(epoch_ajdvs), dtype=torch.float32)\n","        epoch_exp_values_tensor = torch.as_tensor(np.array(epoch_exp_values), dtype=torch.float32)\n","        \n","        self.optimizer.zero_grad()\n","        logits, values = self.model(epoch_obs_tensor)\n","        log_ps = Categorical(logits=logits).log_prob(epoch_actions_tensor)\n","        loss = self.compute_loss(log_ps, values.squeeze(), epoch_adjs_tensor, epoch_exp_values_tensor)\n","        loss.backward()\n","        self.optimizer.step()\n","        \n","        return loss.item()\n","        \n","    # Train the policy model and return training statistics.\n","    def train(self, epochs=50):\n","        epoch_losses = []\n","        for i in range(1, epochs+1):\n","            epoch_loss = self.train_epoch(i)\n","            epoch_losses.append(epoch_loss)\n","\n","    # Evaluate the policy model by acting in the environment for one episode.\n","    def eval(self, render_every: int = 1):\n","        pass"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"DependencyNotInstalled","evalue":"Box2D is not installed, run `pip install gymnasium[box2d]`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/box2d/bipedal_walker.py:15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m         circleShape,\n\u001b[1;32m     18\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         revoluteJointDef,\n\u001b[1;32m     23\u001b[0m     )\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","\u001b[1;32m/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m PolicyGradientTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     max_eps_length \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     n_trajectories_per_epoch \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     method\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39madvantage\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtrain_epoch()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n","\u001b[1;32m/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mreinforce\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcausality\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39madvantage\u001b[39m\u001b[39m'\u001b[39m}:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnsupported method.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_name, render_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m obs_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_Assignment.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m n_acts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n","File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/registration.py:756\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m     env_creator \u001b[39m=\u001b[39m env_spec\u001b[39m.\u001b[39mentry_point\n\u001b[1;32m    754\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 756\u001b[0m     env_creator \u001b[39m=\u001b[39m load_env_creator(env_spec\u001b[39m.\u001b[39;49mentry_point)\n\u001b[1;32m    758\u001b[0m \u001b[39m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    759\u001b[0m render_modes: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/registration.py:545\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 545\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[1;32m    546\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    547\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n","File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1126\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/box2d/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbipedal_walker\u001b[39;00m \u001b[39mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcar_racing\u001b[39;00m \u001b[39mimport\u001b[39;00m CarRacing\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlunar_lander\u001b[39;00m \u001b[39mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n","File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/box2d/bipedal_walker.py:25\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m         circleShape,\n\u001b[1;32m     18\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         revoluteJointDef,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[1;32m     26\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBox2D is not installed, run `pip install gymnasium[box2d]`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     31\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n","\u001b[0;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, run `pip install gymnasium[box2d]`"]}],"source":["trainer = PolicyGradientTrainer(\n","    max_eps_length = 5, \n","    n_trajectories_per_epoch = 5, \n","    method= 'advantage'\n",")\n","loss = trainer.train_epoch()\n","print(loss)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1mA_6yDf22PBDT2INKduSk98AUuESVzwE","timestamp":1700428587570},{"file_id":"1tClkELFzT9WH7GFs-98VXKupmS149qIf","timestamp":1700382749751}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
