{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7853]) <> 0.7852791547775269\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def x_square(x):\n",
    "    return x**2\n",
    "\n",
    "def d_x_square(x):\n",
    "    return 2*x\n",
    "\n",
    "X = torch.rand(1, requires_grad=True)\n",
    "x = X.item()\n",
    "\n",
    "Y = x_square(X)\n",
    "Y.backward()\n",
    "print(X.grad, \"<>\", d_x_square(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn(y_actual, y_pred):\n",
    "    return y_pred - y_actual\n",
    "\n",
    "def d_x_lossfn(y_actual, y_pred, x):\n",
    "    # d(loss)/dx => d(loss)dy * dy/dx\n",
    "    return (0 - 1) * d_x_square(x)\n",
    "\n",
    "y_actual = 10.0\n",
    "X = torch.rand(1, requires_grad=True)\n",
    "x = X.item()\n",
    "Y_pred = x_square(X)\n",
    "y_pred = x_square(x)\n",
    "\n",
    "loss = lossfn(Y_pred, y_actual)\n",
    "loss.backward()\n",
    "print(X.grad, \"<>\", d_x_lossfn(y_actual, y_pred, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-36., dtype=torch.float64) <> -36\n"
     ]
    }
   ],
   "source": [
    "def lossfn2(y_actual, y_pred):\n",
    "    return (y_pred - y_actual) ** 2\n",
    "\n",
    "def d_lossfn2(y_actual, y_pred, x):\n",
    "    # df/dx where f = g**2 and g = y - y_pred\n",
    "    # df/dx = df/dg . dg/dx = 2g . dg/dx\n",
    "    return  2 * lossfn(y_pred, y_actual) * d_x_lossfn(y_actual, y_pred, x)\n",
    "    \n",
    "x = 1\n",
    "y_actual = 10\n",
    "X = torch.tensor(x, dtype=float, requires_grad=True)\n",
    "loss = lossfn2(y_actual, x_square(X))\n",
    "loss.backward()\n",
    "\n",
    "print(X.grad, \"<>\", d_lossfn2(y_actual, x_square(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000, dtype=torch.float64) tensor(1., dtype=torch.float64) <> 2.5 1\n"
     ]
    }
   ],
   "source": [
    "def linear_fn(w, x, b):\n",
    "    return w*x+b\n",
    "\n",
    "def d_w_linear_fn(w, x, b):\n",
    "    # d(w.x+b)/dw = x\n",
    "    return x\n",
    "\n",
    "def d_b_linear_fn(w, x, b):\n",
    "    # d(w.x+b)/db = 1\n",
    "    return 1\n",
    "\n",
    "w = 5\n",
    "b = 4\n",
    "x = 2.5\n",
    "W = torch.tensor(w, dtype=float, requires_grad=True)\n",
    "B = torch.tensor(b, dtype=float, requires_grad=True)\n",
    "X = torch.tensor(x, dtype=float, requires_grad=True)\n",
    "Z = linear_fn(W, x, B)\n",
    "Z.backward()\n",
    "print(W.grad, B.grad, \"<>\", d_w_linear_fn(w, x, b), d_b_linear_fn(w, x, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62., dtype=torch.float64, grad_fn=<AddBackward0>) tensor([1., 5., 6.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([ 2., 10., 12.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return sum(x**2);\n",
    "\n",
    "x = np.array([1.0, 5.0, 6.0])\n",
    "X  = torch.tensor(x, requires_grad=True, dtype=float)\n",
    "Y = f(X)\n",
    "Y.backward()\n",
    "print(Y, X)\n",
    "print(X.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear function z = w.x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B= tensor([[ 0.9501,  0.2477, -1.2972]], requires_grad=True)\n",
      "X= tensor([[-0.5632, -0.3472],\n",
      "        [-1.1954, -0.6511],\n",
      "        [ 1.8192, -1.7200],\n",
      "        [ 0.4597, -0.0819],\n",
      "        [ 0.4172, -0.1399]], requires_grad=True)\n",
      "W= tensor([[0.4458, 0.6325, 0.4701],\n",
      "        [0.0154, 0.1349, 0.2547]], requires_grad=True)\n",
      "torch.Size([5, 2]) torch.Size([2, 3])\n",
      "Q= torch.Size([5, 3]) tensor([[-0.2564, -0.4031, -0.3532],\n",
      "        [-0.5430, -0.8440, -0.7278],\n",
      "        [ 0.7845,  0.9186,  0.4171],\n",
      "        [ 0.2037,  0.2797,  0.1952],\n",
      "        [ 0.1839,  0.2451,  0.1605]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Z= tensor([[ 0.6937, -0.1554, -1.6504],\n",
      "        [ 0.4072, -0.5963, -2.0250],\n",
      "        [ 1.7346,  1.1664, -0.8801],\n",
      "        [ 1.1538,  0.5274, -1.1020],\n",
      "        [ 1.1340,  0.4928, -1.1367]], grad_fn=<AddBackward0>)\n",
      "O= tensor(-0.2362, grad_fn=<SumBackward0>)\n",
      "dz tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "X = torch.randn(5, 2, requires_grad=True)\n",
    "W = torch.randn(2, 3, requires_grad=True)\n",
    "B = torch.randn(1, 3, requires_grad=True)\n",
    "\n",
    "print(\"B=\", B)\n",
    "print(\"X=\", X)\n",
    "print(\"W=\", W)\n",
    "\n",
    "print(X.shape, W.shape)\n",
    "Q = torch.tensordot(X, W, dims=1)\n",
    "Q.retain_grad()\n",
    "print(\"Q=\", Q.shape, Q)\n",
    "\n",
    "Z = Q + B\n",
    "Z.retain_grad()\n",
    "print(\"Z=\", Z)\n",
    "\n",
    "O = torch.sum(Z)\n",
    "print(\"O=\", O)\n",
    "\n",
    "O.backward()\n",
    "#print(\"dz\", Z.grad)\n",
    "print(\"dw\", W.grad)\n",
    "print(\"dx\", X.grad, \"<>\", sum(W[0]), sum(W[1]))\n",
    "print(\"db\", B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive function\n",
    "\n",
    "Computation graph (CS231 course)\n",
    "\n",
    "h(t) = w_h * h(t-1)\n",
    "![comp_graph_recursive_function.jpg](./comp_graph_recursive_function.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) == 2.0 == 2.0\n",
      "tensor(4.) == 4.0 == 4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "W_h = torch.tensor(2.0, requires_grad=True)\n",
    "w_h = W_h.item()\n",
    "\n",
    "H0 = torch.tensor(0.5, requires_grad=True)\n",
    "h0 = H0.item()\n",
    "\n",
    "H1 = W_h * H0\n",
    "h1 = H1.item()\n",
    "\n",
    "H2 = W_h * H1\n",
    "h2 = H2.item()\n",
    "\n",
    "H2.backward()\n",
    "dw_h_over_h0 = 0 #because H0 is constant for w_h\n",
    "dw_h_over_h1 = h0 + w_h * dw_h_over_h0\n",
    "dw_h_over_h2 = h1 + w_h * dw_h_over_h1\n",
    "\n",
    "dh0_over_h0 = 1\n",
    "dh0_over_h1 = w_h * dh0_over_h0\n",
    "dh0_over_h2 = w_h * dh0_over_h1\n",
    "\n",
    "print(W_h.grad, \"<>\", dw_h_over_h2, \"<>\", (h0 * w_h + h1))\n",
    "print(H0.grad, \"<>\", dh0_over_h2, \"<>\", (w_h * w_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://arxiv.org/pdf/1802.01528.pdf\n",
    "\n",
    "https://youtu.be/d14TUNcbn1k?si=hyEeGpEt5hP1XVHA\n",
    "\n",
    "https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
