{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19607,"status":"ok","timestamp":1691674889020,"user":{"displayName":"Soheil Ghafurian","userId":"05631801889436549061"},"user_tz":240},"id":"YZo_arxuE1o_","outputId":"524ed02b-6c19-4f1a-82b7-ed0af9b04a69"},"outputs":[],"source":["import os\n","\n","local_assets_b = False\n","\n","if local_assets_b:\n","  assets_dir = \"/content/assets/P3/\"\n","\n","  if not os.path.isdir(assets_dir):\n","    assert os.path.isfile(\"assets.zip\")\n","    os.system(\"unzip assets.zip\")\n","else:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  assets_dir = '/content/drive/MyDrive/InterviewKickstart/assets/P2/'"]},{"cell_type":"markdown","metadata":{"id":"BsMdG4QTi3vd"},"source":["# Transfer Learning Introduction"]},{"cell_type":"markdown","metadata":{"id":"b5lhD-V5jDdX"},"source":["Transfer learning is a technique in deep learning where pre-trained models trained on large-scale datasets are leveraged to solve new tasks with limited labeled data. It involves taking a pre-trained model, which has learned rich and generalized features from a source task, and fine-tuning it on a target task.\n","\n","### What is VGG?\n","VGG (Visual Geometry Group) is a popular deep convolutional neural network (CNN) architecture developed by the Visual Geometry Group at the University of Oxford. VGGNet is known for its simplicity and effectiveness in image classification tasks. It consists of multiple convolutional layers followed by fully connected layers. The most common variant, VGG-16, has 16 layers, including 13 convolutional layers and 3 fully connected layers. VGGNet has achieved impressive results on various image classification benchmarks, including the ImageNet challenge.\n","\n","### What is CIFAR-10?\n","CIFAR-10 is a widely used benchmark dataset in computer vision and machine learning. It consists of 60,000 small-sized color images (32x32 pixels) belonging to 10 different classes, with 6,000 images per class. The dataset is split into a training set of 50,000 images and a test set of 10,000 images. The classes in CIFAR-10 include common objects like airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. CIFAR-10 serves as a good dataset for evaluating and benchmarking image classification models.\n","\n","When it comes to transfer learning, VGG is often used as a backbone model. Its pre-trained weights, which have been learned on the large-scale ImageNet dataset, capture generic features like edges, textures, and shapes that are beneficial for various visual recognition tasks. By leveraging the pre-trained VGG model, we can fine-tune it on the CIFAR-10 dataset to perform image classification. The lower-level layers of VGG capture low-level features, such as edges and corners, while the higher-level layers learn more complex features. This enables VGG to extract meaningful representations from images and generalize well to new tasks with limited labeled data.\n","\n","By fine-tuning VGG on the CIFAR-10 dataset, we can take advantage of the pre-trained weights and learn task-specific features for image classification. This approach is effective when the target task has a similar domain or visual characteristics as the source task on which VGG was pre-trained. Transfer learning with VGG can help achieve better performance on CIFAR-10 by leveraging the knowledge learned from ImageNet, even with a smaller dataset."]},{"cell_type":"markdown","metadata":{"id":"ze222Um1PkTB"},"source":["## Visualizing different layers of VGG\n","\n","In the VGG network, as the input image progresses through the convolutional layers, it undergoes a series of transformations and feature extractions. One fascinating aspect of VGG is the ability to visualize the learned features at different convolutional layers, which provides insights into the network's inner workings.\n","\n","By examining the feature maps of various convolutional layers, we can observe how the network progressively captures different levels of abstraction. In the early layers, such as the first few convolutional layers, the network tends to learn simple and low-level features like edges, corners, and textures. These features are more local and specific to certain image regions.\n","\n","As we move deeper into the network, the feature maps become more complex and abstract. Higher-level layers capture more global and semantic information, focusing on object parts, shapes, and textures. These learned features are more robust and capable of representing more complex visual patterns.\n","\n","Visualizing the intermediate layers of VGG helps us understand how the network gradually builds a hierarchy of features, with each layer refining and enhancing the representations learned in the previous layers. It also highlights the network's ability to transform raw pixel values into rich, hierarchical feature representations that enable accurate image classification and object detection.\n","\n","By analyzing the visualizations of different conv layers in VGG, we gain valuable insights into the network's feature learning process, offering a glimpse into how deep convolutional neural networks extract and encode meaningful visual information from images. This understanding not only aids in interpreting the network's decisions but also provides a foundation for developing improved architectures and advancing the field of computer vision.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fcpfNurQjCtY"},"source":["### Importing the necessary modules"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4071,"status":"ok","timestamp":1691674900766,"user":{"displayName":"Soheil Ghafurian","userId":"05631801889436549061"},"user_tz":240},"id":"b_zTDJrkdBz8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import matplotlib.pyplot as plt\n","from PIL import Image"]},{"cell_type":"markdown","metadata":{"id":"_CBWnxhrjN8r"},"source":["### Setting up the Transformations for Images"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1691674938931,"user":{"displayName":"Soheil Ghafurian","userId":"05631801889436549061"},"user_tz":240},"id":"zPjkqFu_POTe"},"outputs":[],"source":["transform = transforms.Compose(transforms=[\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"markdown","metadata":{"id":"EZ-jvrNBjYmK"},"source":["### Load the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9541,"status":"ok","timestamp":1691674999584,"user":{"displayName":"Soheil Ghafurian","userId":"05631801889436549061"},"user_tz":240},"id":"VtCVKlgAPPva","outputId":"d3cf5203-0684-4b8b-ee62-f6684c996bcc"},"outputs":[],"source":["# Load the pre-trained VGG model\n","model = models.vgg16(pretrained=True)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"z3TmH7PCjdr8"},"source":["### Extracting the Convolution Layers from VGG"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1691675201558,"user":{"displayName":"Soheil Ghafurian","userId":"05631801889436549061"},"user_tz":240},"id":"lpRjMiY4PVCc","outputId":"7d00dfb8-1c90-4ad0-9a42-c96487be31ef"},"outputs":[],"source":["# we will save the conv layer weights in this list\n","model_weights =[]\n","#we will save the 49 conv layers in this list\n","conv_layers = []\n","# get all the model children as list\n","model_children = list(model.children())\n","#counter to keep count of the conv layers\n","counter = 0\n","#append all the conv layers and their respective wights to the list\n","for i in range(len(model_children)):\n","    print(\"Block: \", i, \" : \",type(model_children[i]))\n","    if isinstance(model_children[i], torch.nn.modules.container.Sequential):\n","        for layer_num, child in enumerate(model_children[i].children()):\n","            print(\"Layer: \", layer_num, \" : \",type(child))\n","            if isinstance(child, nn.Conv2d):\n","                counter+=1\n","                model_weights.append(child.weight)\n","                conv_layers.append(child)\n","print(f\"Total convolution layers found: {counter}\")"]},{"cell_type":"markdown","metadata":{"id":"z6npHvyqjokk"},"source":["### Load the image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"elapsed":2536,"status":"ok","timestamp":1691609540858,"user":{"displayName":"Soheil Ghafurian","userId":"05631801889436549061"},"user_tz":240},"id":"ZbxoYCdCPYvb","outputId":"992b1ca4-2457-4009-e3cf-97ce280eb02f"},"outputs":[],"source":["image = Image.open(fp = assets_dir + 'dog_image.jpeg')\n","plt.imshow(image)\n","image = transform(image)\n","# Reshape the image to match the input size of VGG\n","print(f\"Image shape before: {image.shape}\")\n","image = image.unsqueeze(0)\n","print(f\"Image shape after: {image.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"ZPfV9n9zjsER"},"source":["### Run the image on the Extracted Convolution Layers and Visualise them"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"L_w260Q7PgPb","outputId":"a2bfaa64-3711-4cdb-f19a-65f1348eb5e3"},"outputs":[],"source":["# Set the model to evaluation mode\n","model.eval()\n","\n","results = [conv_layers[0](image)]\n","for i in range(1, len(conv_layers)):\n","    results.append(conv_layers[i](results[-1]))\n","outputs = results\n","\n","for num_layer in range(len(outputs)):\n","    plt.figure(figsize=(50, 10))\n","    layer_viz = outputs[num_layer][0, :, :, :]\n","    layer_viz = layer_viz.data\n","    print(\"Layer \",num_layer+1)\n","    for i, filter in enumerate(layer_viz):\n","        if i == 16:\n","            break\n","        plt.subplot(2, 8, i + 1)\n","        plt.imshow(filter, cmap='gray')\n","        plt.axis(\"off\")\n","    plt.show()\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"oQxrW_4VRSOW"},"source":["## Transfer Learning vs from Scratch\n","\n","Below we will develop 2 models to show the benefits of using Transfer Learning. Transfer Learning will help us save time (which is very valuable) and cost (computation required is less, equally valuable)."]},{"cell_type":"markdown","metadata":{"id":"fS16lIq9PiFu"},"source":["### Using VGG for Transfer Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TIFRLx9fTZ6G"},"outputs":[],"source":["# define the imports\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import vgg16"]},{"cell_type":"markdown","metadata":{"id":"hID8dtbrj4kj"},"source":["The below code defines a transformation pipeline for the CIFAR-10 dataset. It resizes the images to a size of 224x224 pixels, converts them to tensors, and then applies normalization with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5). These transformations are commonly used to preprocess the CIFAR-10 images before feeding them into the VGG model for training or inference."]},{"cell_type":"markdown","metadata":{"id":"YFSHEb1MzAxL"},"source":["### Setting up the Transformations for Images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bTpcIYZJTpG_"},"outputs":[],"source":["# Define the transformations\n","transform = transforms.Compose(transforms=[\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"]},{"cell_type":"markdown","metadata":{"id":"Qns3DjpyyX2c"},"source":["### Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"31eAk-4aT4c2","outputId":"40eae108-56a6-45e4-b576-fb7a4e79b688"},"outputs":[],"source":["import os\n","\n","cifar_root_dir = './.data/cifar-10'\n","os.makedirs(cifar_root_dir, exist_ok=True)\n","\n","# Load the CIFAR-10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root=cifar_root_dir, train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.CIFAR10(root=cifar_root_dir, train=False, download=True, transform=transform)"]},{"cell_type":"markdown","metadata":{"id":"Tr6N3Ledl3Tu"},"source":["### Datasets and DataLoaders\n","\n","\n","In PyTorch, datasets and dataloaders are essential components for handling and processing data during the training and testing phases of machine learning models.\n","\n","Datasets represent collections of data samples with their corresponding labels. They provide an interface to access individual samples, allowing users to retrieve and preprocess data for training or evaluation. PyTorch provides built-in datasets like MNIST, CIFAR-10, and ImageNet, but custom datasets can also be created to work with specific data.\n","\n","Dataloaders, on the other hand, are utilities that enable efficient data loading and batching. They take a dataset as input and allow users to define batch sizes, shuffle the data, and apply transformations to the samples. Dataloaders are especially useful when dealing with large datasets, as they enable the model to process data in small batches, reducing memory requirements and speeding up training. They are key components in PyTorch that facilitate data handling and preparation for machine learning tasks."]},{"cell_type":"markdown","metadata":{"id":"b6HIzlKbOgN-"},"source":["Since the CIFAR-10 dataset is huge, we will be filtering it to contain only 2 classes, cats and dogs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wzU_UPoyT_J9"},"outputs":[],"source":["# Define the specific classes you want to train on\n","class_names = ['dog', 'cat']\n","# These are the ids of the dog and cat class in the CIFAR-10 dataset\n","class_ids = [3, 5]\n","# We are mapping the class labels to to 0 and 1 as the output layer will have these indices and it makes it easier to evaluate the predictions\n","label_map = {3:0, 5:1}\n","\n","# Filter the dataset to include only the selected classes\n","train_indices = [idx for idx, label in enumerate(train_dataset.targets) if label in class_ids]\n","test_indices = [idx for idx, label in enumerate(test_dataset.targets) if label in class_ids]\n","\n","\n","train_dataset = torch.utils.data.Subset(dataset=train_dataset, indices=train_indices)\n","test_dataset = torch.utils.data.Subset(dataset=test_dataset, indices=test_indices)"]},{"cell_type":"markdown","metadata":{"id":"fFK4wH3pzGNG"},"source":["### Defining the DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fHJOl_5XUSDz"},"outputs":[],"source":["# Define the data loaders\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"ViF-umPzOkN-"},"source":["Since the last layer of VGG is 1000 (as it was trained for ImageNet which contains 1000 classes) we are removing that and connecting the second last layer to the number of classes we currently have i.e. 2."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"H1vUR9ALUfK4","outputId":"7472a3ed-5413-442a-b17e-828b9fced808"},"outputs":[],"source":["# Load the pre-trained VGG-16 model\n","vgg = vgg16(pretrained=True)\n","# Modify the last layer of VGG by changing it to 2 classes instead of 1000 as trained for ImageNet\n","vgg.classifier[6] = nn.Linear(in_features=4096, out_features=len(class_names))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","vgg.to(device)"]},{"cell_type":"markdown","metadata":{"id":"4fFpxYwGoUSU"},"source":["### Loss Function and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lhHagozPUjGK"},"outputs":[],"source":["# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(params=vgg.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"g3GRPlHLoQ4i"},"source":["### Training the Model\n","\n","Note: The below code takes around 10 mins to run, so you can load the model in case you would like to play around without waiting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUzxY1GFzZ8C"},"outputs":[],"source":["# Training loop\n","num_epochs = 4\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        # Convert class labels to numeric values i.e. dog(3) to 0 and cat(5) to 1 so that they are mapped to the output layer\n","        transformed_tensor = list(map(torch.tensor, [label_map[val.item()] for val in labels]))\n","        labels = torch.tensor(transformed_tensor)\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad() # Clear gradients from previous iteration\n","        outputs = vgg(images) # Forward pass to get model predictions\n","        loss = criterion(outputs, labels) # Calculate the loss between predictions and actual labels\n","        loss.backward() # Backpropagation to compute gradients\n","        optimizer.step() # Update model parameters using the computed gradients\n","        running_loss += loss.item() # Accumulate training loss\n","\n","    # Calculate accuracy on the test set\n","\n","    # Set the VGG model to evaluation mode (no gradients)\n","    vgg.eval()\n","    # Initialize variables to keep track of correct and total predictions\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        # Iterate through the test_loader, which contains test images and their corresponding labels\n","        for images, labels in test_loader:\n","            # Convert class labels to numeric values i.e. dog(3) to 0 and cat(5) to 1 so that they are mapped to the output layer\n","            transformed_tensor = list(map(torch.tensor, [label_map[val.item()] for val in labels]))\n","            labels = torch.tensor(transformed_tensor)\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = vgg(images)\n","            # Get the predicted class with the highest probability for each image\n","            _, predicted = torch.max(outputs.data, 1)\n","            # Update the total count of test samples\n","            total += labels.size(0)\n","            # Count the number of correct predictions by comparing predicted labels with true labels\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss:.4f} - Test Accuracy: {accuracy:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"6j5HYJPZGzBW"},"source":["### Save your Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBpF4S3Qvvv9"},"outputs":[],"source":["import os\n","\n","cifar_modelstate_root_dir = './.model-state/cifar-10'\n","os.makedirs(cifar_modelstate_root_dir, exist_ok=True)\n","\n","# Uncomment the below code in case you want to save your model\n","\"\"\"\n","# Save the model\n","state_dict = vgg.state_dict()\n","torch.save(state_dict, cifar_modelstate_root_dir+\"/vgg_model_state_dict.pt\")\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"wtq3aj26G1dF"},"source":["### Load your already saved model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB7NWVg8vzGy"},"outputs":[],"source":["from torchvision.models import vgg16\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Load the model in case training it takes a lot of time\n","file_path = cifar_modelstate_root_dir+\"/vgg_model_state_dict.pt\"\n","\n","loaded_vgg_model = vgg16(pretrained=False)\n","loaded_vgg_model.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n","# Load the saved state dictionary\n","saved_state_dict = torch.load(file_path)\n","# Load the state dictionary into the model\n","loaded_vgg_model.load_state_dict(saved_state_dict)\n","# Set the model to evaluation mode\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loaded_vgg_model = loaded_vgg_model.to(device)\n","loaded_vgg_model.eval()"]},{"cell_type":"markdown","metadata":{"id":"wqfBionNn7EN"},"source":["### Evaluating the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4ElmnpntUfL"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","def evaluate(model, data_loader):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            # Convert class labels to numeric values i.e. dog(3) to 0 and cat(5) to 1 so that they are mapped to the output layer\n","            transformed_tensor = list(map(torch.tensor, [label_map[val.item()] for val in labels]))\n","            labels = torch.tensor(transformed_tensor)\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            # Get the predicted class with the highest probability for each image\n","            _, predicted_labels = torch.max(outputs, 1)\n","\n","            # Append the predictions\n","            predictions.extend(predicted_labels.cpu().numpy())\n","            # Append the true labels\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    # Use the predictions and true labels to evaluate the model on the following\n","    accuracy = accuracy_score(y_true=true_labels, y_pred=predictions)\n","    precision = precision_score(y_true=true_labels, y_pred=predictions, average='weighted')\n","    recall = recall_score(y_true=true_labels, y_pred=predictions)\n","    f1 = f1_score(y_true=true_labels, y_pred=predictions)\n","    cf_matrix = confusion_matrix(y_true=true_labels, y_pred=predictions)\n","\n","    return accuracy, precision, recall, f1, cf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okMmelSyD5iZ"},"outputs":[],"source":["# Note: Uncomment the below line in case you are using the trained model or else use the one below it with the loaded model\n","# test_accuracy, test_precision, test_recall, test_f1, cf_matrix = evaluate(vgg, test_loader)\n","test_accuracy, test_precision, test_recall, test_f1, cf_matrix = evaluate(loaded_vgg_model, test_loader)\n","\n","# Print final test set performance\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Precision: {test_precision:.4f}\")\n","print(f\"Test Recall: {test_recall:.4f}\")\n","print(f\"Test F1 Score: {test_f1:.4f}\")\n","print(f\"Test Confusion Matrix:\")\n","print(cf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"o3evm6mJOpUK"},"source":["### Developing a model from Scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbJi6WoWalA8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import CIFAR10\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdpINLBzayL6"},"outputs":[],"source":["# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"P5oLhETqp2tw"},"source":["### CNN Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"EhMIWTpmOrHq"},"source":["The below CNN class is an architecture we are building from scratch in order to compare which one performs better in the task of Image Classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aP68UfobBgE"},"outputs":[],"source":["# Define model architecture\n","class CNN(nn.Module):\n","    \"\"\"\n","    Convolutional Neural Network (CNN) for image classification.\n","\n","    This CNN consists of two convolutional layers followed by max-pooling layers,\n","    and two fully connected layers. The input images are expected to have three channels (RGB).\n","\n","    Args:\n","        num_classes (int): The number of classes in the classification task.\n","\n","    Attributes:\n","        conv1 (nn.Conv2d): The first convolutional layer with 16 output channels and a kernel size of 3x3.\n","        relu1 (nn.ReLU): The ReLU activation function applied after the first convolutional layer.\n","        pool1 (nn.MaxPool2d): The max-pooling layer with a kernel size of 2x2 after the first convolutional layer.\n","        conv2 (nn.Conv2d): The second convolutional layer with 32 output channels and a kernel size of 3x3.\n","        relu2 (nn.ReLU): The ReLU activation function applied after the second convolutional layer.\n","        pool2 (nn.MaxPool2d): The max-pooling layer with a kernel size of 2x2 after the second convolutional layer.\n","        fc1 (nn.Linear): The first fully connected layer with 64 units.\n","        relu3 (nn.ReLU): The ReLU activation function applied after the first fully connected layer.\n","        fc2 (nn.Linear): The second fully connected layer with `num_classes` units for classification.\n","\n","    Methods:\n","        forward(x): Performs a forward pass through the network given an input tensor x.\n","                    Returns the output tensor after passing through the fully connected layers.\n","    \"\"\"\n","\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(in_features=32 * 8 * 8, out_features=64)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool1(self.relu1(self.conv1(x)))\n","        x = self.pool2(self.relu2(self.conv2(x)))\n","        x = x.view(-1, 32 * 8 * 8)\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"WCTaAHdgqNOJ"},"source":["### Training Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7bizeGBbSH2"},"outputs":[],"source":["# Define training parameters\n","batch_size = 32\n","learning_rate = 0.001"]},{"cell_type":"markdown","metadata":{"id":"rCtr-Ib5qRXO"},"source":["### Transformations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFqzW_nAbk5r"},"outputs":[],"source":["# Define the transformations\n","transform = transforms.Compose(transforms=[\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"]},{"cell_type":"markdown","metadata":{"id":"H03meoynqW7_"},"source":["### Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcJjlF_WcMyh"},"outputs":[],"source":["# Load the CIFAR-10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"]},{"cell_type":"markdown","metadata":{"id":"1g5OTjEeqmQ-"},"source":["### Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wh9Wmb6RcR4Q"},"outputs":[],"source":["# Define the specific classes you want to train on\n","class_names = ['dog', 'cat']\n","# These are the ids of the dog and cat class in the CIFAR-10 dataset\n","class_ids = [3, 5]\n","# We are mapping the class labels to to 0 and 1 as the output layer will have these indices and it makes it easier to evaluate the predictions\n","label_map = {3:0, 5:1}\n","\n","# Filter the dataset to include only the selected classes\n","train_indices = [idx for idx, label in enumerate(train_dataset.targets) if label in class_ids]\n","test_indices = [idx for idx, label in enumerate(test_dataset.targets) if label in class_ids]\n","\n","\n","train_dataset = torch.utils.data.Subset(dataset=train_dataset, indices=train_indices)\n","test_dataset = torch.utils.data.Subset(dataset=test_dataset, indices=test_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v88bYIamcY1o"},"outputs":[],"source":["# Define the data loaders\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"MPpj5l1mrkEs"},"source":["### Loss Criterion and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFzTUiFTczca"},"outputs":[],"source":["# Create the model\n","model = CNN(num_classes=2).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"CzH1N8v5rdy4"},"source":["### Training the model and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuLlcpHsf6rf"},"outputs":[],"source":["# Training loop\n","num_epochs = 4\n","for epoch in range(num_epochs):\n","    model.train() # Set the model to training mode so that it can learn from the training data\n","    train_loss = 0.0\n","    for images, labels in train_loader:\n","        # Convert class labels to numeric values i.e. dog(3) to 0 and cat(5) to 1 so that they are mapped to the output layer\n","        transformed_tensor = list(map(torch.tensor, [label_map[val.item()] for val in labels]))\n","        labels = torch.tensor(transformed_tensor)\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad() # Clear gradients from previous iteration\n","        outputs = model(images) # Forward pass to get model predictions\n","        loss = criterion(outputs, labels) # Calculate the loss between predictions and actual labels\n","        loss.backward() # Backpropagation to compute gradients\n","        optimizer.step() # Update model parameters using the computed gradients\n","        train_loss += loss.item() * images.size(0) # Accumulate training loss\n","    train_loss /= len(train_loader.dataset)\n","\n","    # Evaluation on the test set\n","    model.eval()\n","    test_loss = 0.0\n","    true_labels = []\n","    pred_labels = []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            # Convert class labels to numeric values i.e. dog(3) to 0 and cat(5) to 1 so that they are mapped to the output layer\n","            transformed_tensor = list(map(torch.tensor, [label_map[val.item()] for val in labels]))\n","            labels = torch.tensor(transformed_tensor)\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item()\n","            # Get the predicted class with the highest probability for each image\n","            _, predicted = torch.max(outputs, 1)\n","            # Append the true labels and the predicted labels to their respective lists\n","            true_labels.extend(labels.cpu().numpy())\n","            pred_labels.extend(predicted.cpu().numpy())\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    # Calculate evaluation metrics based on the true labels and the predicted labels\n","    accuracy = accuracy_score(y_true=true_labels, y_pred=pred_labels)\n","    precision = precision_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n","    recall = recall_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n","    f1 = f1_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n","\n","    # Print the evaluation metrics\n","    print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Test Loss: {test_loss:.4f} - \"\n","          f\"Accuracy: {accuracy:.4f} - \"\n","          f\"Precision: {precision:.4f} - \"\n","          f\"Recall: {recall:.4f} - \"\n","          f\"F1 Score: {f1:.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
