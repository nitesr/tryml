{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# AutoEncoders for Supervised Anomaly Detection"],"metadata":{"id":"MfW9kk5f4AuK"}},{"cell_type":"markdown","source":["## Anomaly Detection: Detecting the Unusual"],"metadata":{"id":"nB7_BZcs5z1N"}},{"cell_type":"markdown","source":["Anomaly detection is a crucial task in various domains, where the objective is to identify rare and abnormal instances that deviate significantly from the expected or normal behavior. It plays a vital role in fraud detection, network security, system monitoring, and predictive maintenance."],"metadata":{"id":"P27H9Eq1582q"}},{"cell_type":"markdown","source":["## Autoencoders for Unsupervised Anomaly Detection"],"metadata":{"id":"N7hWLq-N5_D1"}},{"cell_type":"markdown","source":["In the unsupervised setting, autoencoders are trained to reconstruct the input data accurately. The idea is that the model learns to encode the normal instances in a compressed representation and then decodes them back to their original form. Anomalies, being different from the normal patterns, are expected to have higher reconstruction errors. By setting a suitable threshold on the reconstruction error, anomalies can be identified."],"metadata":{"id":"siq27OcB6HNH"}},{"cell_type":"markdown","source":["## Incorporating Supervision with Autoencoder Features"],"metadata":{"id":"hhGw6NaZ6MVB"}},{"cell_type":"markdown","source":["**In this exercise, we will explore a different approach by incorporating supervision into the anomaly detection process. We will still employ autoencoders, but instead of relying solely on the reconstruction error for anomaly detection, we will leverage the learned features from the encoder part of the autoencoder and utilize them to learn a supervised classifier to detect anomalies.**\n","\n","In this exercise you will:\n","\n","\n","\n","*   Learn how to train AutoEncoders on Relational Data.\n","*   Learn how compressed representations from an AutoEncoder can help achieve performance comparable to original dimensions on downstream task.\n","*   Learn how to use AutoEncoder for Supervised Anomaly Detection tasks.\n","\n","\n","\n"],"metadata":{"id":"hDXh3_La6RDg"}},{"cell_type":"markdown","source":["*This notebook is designed to help you guide how to approach this assignment.*\n","\n","<i><font color='blue'>Some parts of the notebook are left as exercise for you and are the corresponding headers are marked in blue</font></i>"],"metadata":{"id":"tYt0_TB7FKo8"}},{"cell_type":"markdown","source":["# Exploring the Dataset"],"metadata":{"id":"kHaTng5d7K24"}},{"cell_type":"markdown","source":["* Create a directory called **kdd-data** in the root (**My Drive**) of your Google Drive.\n","* Upload the provided **kdd-data.zip** to this **kdd-data** directory."],"metadata":{"id":"Zz7p4g7b9DGK"}},{"cell_type":"markdown","source":["## Importing the required libraries"],"metadata":{"id":"BivsqqG_BSgx"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras import layers, Model, models\n","from sklearn.metrics import classification_report, confusion_matrix\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.optimizers import Adam\n","import seaborn as sns"],"metadata":{"id":"iNr-OF6nBhK2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Mount the Google Drive"],"metadata":{"id":"xGd8eaou92DF"}},{"cell_type":"markdown","source":["After mounting the drive, the **kdd-data.zip** should appear at the following path in the left (**Files**) pane in Colab: **/content/gdrive/MyDrive/kdd-data/kdd-data.zip.**\n","\n","If it doesn't, make sure you recheck and follow the steps mentioned above."],"metadata":{"id":"IYTxeU8i984x"}},{"cell_type":"code","source":["drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"7cOyH6GVgoKo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Upzip the Data Zip Archive"],"metadata":{"id":"aX4dL2KP-mPV"}},{"cell_type":"code","source":["!unzip /content/gdrive/MyDrive/kdd-data/kdd-data.zip -d /content/gdrive/MyDrive/kdd-data/"],"metadata":{"id":"GXcTRi7Agy4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The uncompressed data should now be available under **/content/gdrive/MyDrive/kdd-data/**"],"metadata":{"id":"IZryxw5v-shw"}},{"cell_type":"markdown","source":["## Description of the KDD Cup 1999 Data"],"metadata":{"id":"9yF_dfax-4XD"}},{"cell_type":"markdown","source":["The KDD Cup 1999 dataset is a widely used benchmark dataset in the field of anomaly detection and network security. It was created as part of the Third International Knowledge Discovery and Data Mining Tools Competition, held in 1999.\n","\n","The dataset contains a large set of network traffic data captured from various sources, simulating a network environment. It includes a wide range of features that represent different aspects of network connections, such as duration, protocol type, service, flag, source and destination bytes, and more.\n","\n","The primary goal of the KDD Cup 1999 dataset is to classify network connections as either \"normal\" or \"anomalous.\"\n","\n","The dataset includes a categorical label column called \"label,\" which categorizes network connections into various attack types, such as \"normal,\" \"dos\" (denial-of-service), \"probe\" (scanning and probing), \"r2l\" (unauthorized access from a remote machine), and \"u2r\" (unauthorized access to local root privileges)."],"metadata":{"id":"kdMvHerC_QSs"}},{"cell_type":"markdown","source":["## Loading the dataset"],"metadata":{"id":"L9bBdDel_zz5"}},{"cell_type":"markdown","source":["The KDD Dataset contains around 5 million instances of network traffic data. Each instance represents a network connection with various features and a corresponding label.\n","\n","Due to the resource limitations in Colab, we will utilize a smaller subset of the data by using a 10% split (kddcup.data_10_percent) for training our models. This subset will allow us to work within the resource constraints while still providing enough data for training and evaluation.\n","\n","The description of the dataset, along with the features is available [here](https://kdd.ics.uci.edu/databases/kddcup99/task.html)."],"metadata":{"id":"xn2oA8S1_64x"}},{"cell_type":"markdown","source":["The dataset file doesn't contain any header. Therefore loading the header information about the dataset as described [here](https://kdd.ics.uci.edu/databases/kddcup99/kddcup.names)."],"metadata":{"id":"4Nx73A03CQUK"}},{"cell_type":"code","source":["# Load the KDD Cup 1999 dataset\n","\n","data = pd.read_csv('/content/gdrive/MyDrive/kdd-data/kddcup.data_10_percent/kddcup.data_10_percent', header=None)"],"metadata":{"id":"cz9dsBtl_zJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the column names\n","column_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n","                \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\",\n","                \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n","                \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n","                \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n","                \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n","                \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n","                \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n","                \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\n","                \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n","                \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n","                \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\",\n","                \"dst_host_srv_rerror_rate\", \"label\"]\n","\n","# Assign the column names to the dataset\n","data.columns = column_names"],"metadata":{"id":"ehmK5ZWImYq8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Checkout the first few rows of the dataset"],"metadata":{"id":"S6INqNJVD-N7"}},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)\n","data.head(10)"],"metadata":{"id":"P0IFECBrEHoi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Label Fequency"],"metadata":{"id":"iJ22YjxfH7CX"}},{"cell_type":"markdown","source":["<font color='blue'>Create a Bar Plot to check the frequency for each label in the dataset</font>"],"metadata":{"id":"J8J48Wv2ICVe"}},{"cell_type":"code","source":["# def plot_value_count(data):\n","\n","# plot_value_count(data)"],"metadata":{"id":"b5oSnbRmITZ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### All classes except \"normal.\" in this dataset are considered attacks. They can be considered anomalies in our case."],"metadata":{"id":"12eUlsptInrP"}},{"cell_type":"markdown","source":["Since our goal is to classify the network traffic as \"normal\" or \"anomalous\", we just need to convert the remaining classes into these two, and project this as a binary classification problem.\n","\n","<font color='blue'>Convert all \"normal\" labels to a value \"0\", while all other labels to a value \"1\". Update the same \"label\" column in the \"data\" dataframe with the new values.</font>"],"metadata":{"id":"9weWMT_YI1Mo"}},{"cell_type":"code","source":["# data[\"label\"] ="],"metadata":{"id":"3kxF1mNeJ0K2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at the label frequency now"],"metadata":{"id":"Q4O2ak8HKenq"}},{"cell_type":"code","source":["# the \"label\" column in the data df should contain the two labels \"0\" and \"1\" now.\n","plot_value_count(data)"],"metadata":{"id":"CDj6YYT8KjOQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since we have the labels available for anomalous and non amomalous datapoints, this dataset is a good candidate for supervised anomaly detection.\n","\n","Note that anomaly detection datasets usually have \"less\" anomalous data points than non-anomalous dataset, however, this is not the case in this dataset, even without binarizing the labels. Since, the dataset has **significant** number of anomalous datapoints, this also makes it a good candidate for Supervised Anomaly Detection."],"metadata":{"id":"kjQJvOSEMYtO"}},{"cell_type":"markdown","source":["## Splitting the dataset before Preprocessing"],"metadata":{"id":"SfC8q3thNkW9"}},{"cell_type":"code","source":["# Separate the labels from the features\n","X = data.drop(\"label\", axis=1)\n","y = data[\"label\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"kuVGcOydOomv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Processing the Categorical Features"],"metadata":{"id":"PKSm4baCGeSG"}},{"cell_type":"markdown","source":["Let's look at the dataset description and figure out all the categorical features available in the dataset.\n","\n","\n","\n"],"metadata":{"id":"UGwAQ5oWGTRF"}},{"cell_type":"code","source":["categorical_features = [\"protocol_type\", \"service\", \"flag\"]"],"metadata":{"id":"Jqb6_XdzGoTp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Now convert the categorical features in One Hot representation. The code should update the X_train and X_test dataframes, with the one hot encoded features, and the original features dropped.</font>"],"metadata":{"id":"4G3zPAeGGs50"}},{"cell_type":"code","source":["# Initialize the OneHotEncoder\n","\n","# Fit and transform the categorical features for training data\n","\n","# Transform the categorical features for test data\n","\n","# Create DataFrames with the one-hot encoded features\n","\n","# Drop the original categorical features\n","\n","# Concatenate the one-hot encoded features with the original datasets"],"metadata":{"id":"qWuNhKaAS2OD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's look at the new shape of our encoded dataset"],"metadata":{"id":"Bulzib4CTmgA"}},{"cell_type":"code","source":["print(X_train.shape)\n","print(X_test.shape)"],"metadata":{"id":"1mgK2mCtTbAl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Normalizing the Continous Features"],"metadata":{"id":"dKQM3IsjYfOZ"}},{"cell_type":"markdown","source":["The continous (non-categorical) features in the KDD dataset have different scales. This can be a problem for our models. Let's normalize these features."],"metadata":{"id":"Q4oF4tqwaIjE"}},{"cell_type":"code","source":["continuous_features = [x for x in column_names if x not in categorical_features and x !='label']\n","print('Total number of non-categorical features: ', len(continuous_features))"],"metadata":{"id":"bTeTyAK4ZK2Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Write code to normalize the non categorical features in the dataset. The code should update X_train and X_test dataframes.</font>"],"metadata":{"id":"pefF9SIVaUpS"}},{"cell_type":"code","source":["# Initialize the MinMaxScaler\n","\n","# Fit and transform the continuous features in X_train\n","\n","# Transform the continuous features in X_test\n","\n","# Display the normalized datasets"],"metadata":{"id":"7MkFUU-NaoU8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelling"],"metadata":{"id":"0vyiQ4XAc3y-"}},{"cell_type":"markdown","source":["## AutoEncoder"],"metadata":{"id":"49MIf0SSdB5V"}},{"cell_type":"markdown","source":["Now that the train and test data is ready. Let's design an AutoEncoder."],"metadata":{"id":"l4VHAFwwc7wo"}},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"s5Sek2yedE0h"}},{"cell_type":"markdown","source":["<font color='blue'>Design an Encoder. Take any aribitrary number of Dense Layers, with arbitrary number of neurons in each layers. The output dimensions should be significantly less than the number of dimensions in the Training Dataset. Feel free to try out various architectures here and use the one yeilding best results. The encoder should be an object of \"Model\" class</font>"],"metadata":{"id":"nxoke-JcdINk"}},{"cell_type":"code","source":["# Define/Initialize the input dimension\n","\n","# Define the latent space dimension\n","\n","# Define the layers of the Encoder\n","\n","# Initialize the encoder Model using the above created architecture\n","\n","# Print encoder summary"],"metadata":{"id":"UZ2ZJTuydxhp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Design the decoder. The architecture of the Decoder should complement the Encoder. The decoder should be an object of \"Model\" class</font>"],"metadata":{"id":"29gOB601epYA"}},{"cell_type":"code","source":["# Define the layers of the Decoder\n","\n","# Initialize the Decoder Model using the above created architecture\n","\n","# Print decoder summary"],"metadata":{"id":"kW-RzGEGemwJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Combine encoder and decoder to produce the final model.</font>"],"metadata":{"id":"tclDerz9fI8q"}},{"cell_type":"code","source":["# Combine encoder and decoder models to create the Autoencoder model"],"metadata":{"id":"7LfDFKm9fOQ5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Compile and Train the AutoEncoder on X_train. Feel free to try out various hyperparameters, and/or try various hyperparameter search techniques.</font>"],"metadata":{"id":"XvZSkYWugrBk"}},{"cell_type":"code","source":["# Compile the autoencoder\n","\n","# Train the autoencoder"],"metadata":{"id":"-MIr16cahAXO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Create a plot to visualize the training and validation loss</font>"],"metadata":{"id":"FWTA1n4gh5gm"}},{"cell_type":"code","source":["# Access the loss history\n","\n","# Plot the training and validation loss curves"],"metadata":{"id":"NuHcM2lViD4V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classifier"],"metadata":{"id":"09pwYNPDicsO"}},{"cell_type":"markdown","source":["The AutoEncoder can now give us a latent space from the Encoder, with the features representing the maximum information from the original set of 117 features. The task is to build a classifier, use this latent space from the AutoEncoder as input features to the classifier, and see if the classifier can predict network attacks accurately."],"metadata":{"id":"OhYgTXTOie0K"}},{"cell_type":"markdown","source":["<font color='blue'> Build a classifier here. You are free to use any architecture i.e. non-neural models such as Logistic Regression, Decision Trees, Random Forest etc. or a Sequential Feed Forward Neural Network. </font>"],"metadata":{"id":"JYH-LVA8jJHt"}},{"cell_type":"code","source":["# Use the following function definition for the classifier. Feel free to add more hyperparameters to the function parameters, if needed.\n","# def get_classifier(input_dimension, lr):\n","\n","  # return classifier"],"metadata":{"id":"JFyjJ0Unj3C9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Classification on the non-autoencoded dataset"],"metadata":{"id":"1ktA705slP47"}},{"cell_type":"markdown","source":["<font color='blue'>Train the classifier on the original data first - i.e. on X_train. You do not need to worry about the AutoEncoder's output in this exercise.</font>"],"metadata":{"id":"ZjC7DsrhkKYv"}},{"cell_type":"code","source":["# Get the classifier object using the above function defintion\n","\n","# Train the classifier model on the raw features. Use X_test as validation data"],"metadata":{"id":"KfdEE5JtkZNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Generate predictions on the X_test, Generate Classification Report and plot confusion matrix</font>"],"metadata":{"id":"acGpNZzzkmWr"}},{"cell_type":"code","source":["# Predict on the test set\n","\n","# Evaluate the classifier by generating the classification report."],"metadata":{"id":"OuQ_DpHY2Ipk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate the confusion matrix\n","\n","# Plot the confusion matrix"],"metadata":{"id":"e2MnT6EilLS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Classification on the Autoencoded dataset"],"metadata":{"id":"dlFej2OhlcjB"}},{"cell_type":"markdown","source":["<font color='blue'>Write code to generate latent representations for X_train and X_test from the Encoder component of the AutoEncoder</font>"],"metadata":{"id":"S4KY-pDylfUz"}},{"cell_type":"code","source":["# Obtain the encoded features on X_train\n","\n","# Obtain the encoded features on X_test"],"metadata":{"id":"k70e-Q8X3L8b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Now train the classifier on these latent representations. You should try out various hyperparameters which your classifier can take in order to achieve the best performance.</font>"],"metadata":{"id":"N_3VvBcSmYBP"}},{"cell_type":"code","source":["# Get the classifier object using the above function defintion\n","\n","# Train the classifier model on the encoded features obtained from the AutoEncoder. Use encoded features obtained for X_test as validation data."],"metadata":{"id":"ODRyM9lkmVMg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>Generate predictions on the X_test, Generate Classification Report and plot confusion matrix.</font>"],"metadata":{"id":"n2Z7mXI0mnmu"}},{"cell_type":"code","source":["# Predict on the test set\n","\n","# Evaluate the classifier"],"metadata":{"id":"QqYc3CSnqfQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the confusion matrix\n","\n","# Plot the confusion matrix"],"metadata":{"id":"JMVhwI0cqU8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Summary"],"metadata":{"id":"zZhxfzyumthP"}},{"cell_type":"markdown","source":["<font color='blue'>Summarize your observations when training the classifier on the Raw Features vs training it on the encoded features.\n","\n","Compare and Summarize the results of the two approaches.</font>"],"metadata":{"id":"Jkf2wQjSmvoL"}}]}