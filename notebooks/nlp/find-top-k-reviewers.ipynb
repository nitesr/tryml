{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCfDA96yGs2L"
      },
      "outputs": [],
      "source": [
        "!pip3 install numpy\n",
        "!pip3 install matplotlib\n",
        "!pip3 install pandas\n",
        "!pip3 install pymupdf\n",
        "!pip3 install PyPDF2\n",
        "!pip3 install pytorch\n",
        "!pip3 install gensim\n",
        "!pip3 install nltk\n",
        "!pip3 install scipy\n",
        "!pip3 install sklearn\n",
        "!pip3 install lxml\n",
        "!pip3 install requests beautifulsoup4\n",
        "!pip3 install google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem\n",
        "Design a system that takes a research paper (pdf) as input and generates a list of the “Top k authors” (the author can specify k) that can potentially review their paper."
      ],
      "metadata": {
        "id": "7o_aqJ24GupM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
        "\n",
        "List files:\n",
        "gsutil cp gs://arxiv-dataset/arxiv/\n",
        "\n",
        "Download pdfs from March 2020:\n",
        "gsutil cp gs://arxiv-dataset/arxiv/arxiv/pdf/2003/ ./a_local_directory/\n",
        "\n",
        "Download all the source files\n",
        "gsutil cp -r gs://arxiv-dataset/arxiv/  ./a_local_directory/\n",
        "\n",
        "#Get 10 documents\n",
        "```\n",
        "sample_docs = [\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00001v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00003v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00004v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00005v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00006v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00007v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00007v2.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00008v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00009v1.pdf'\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "P-SxQ39uGyYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "meta_url=\"https://arxiv.org/abs/{}\"\n",
        "pdf_url=\"https://arxiv.org/pdf/{}.pdf\"\n",
        "\n",
        "def extract_metadata(paper_id):\n",
        "  # Make a GET request to fetch the raw HTML content\n",
        "  html_content = requests.get(meta_url.format(paper_id)).text\n",
        "\n",
        "  # Parse the html content\n",
        "  soup = BeautifulSoup(html_content, \"lxml\")\n",
        "  paper_title = soup.find(\"h1\", attrs={\"class\": (\"title\", \"mathjax\")}).contents[1]\n",
        "  soup_authors = soup.find(\"div\", attrs={\"class\": \"authors\"})\n",
        "  paper_authors = []\n",
        "  for soup_author in soup_authors.find(\"a\"):\n",
        "    paper_authors.append(soup_author.text)\n",
        "  paper_abstract = soup.find(\"blockquote\", attrs={\"class\": (\"abstract\", \"mathjax\")}).contents[2]\n",
        "  return (paper_title, paper_authors, paper_abstract)"
      ],
      "metadata": {
        "id": "GgnM9SAJPGH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "sample_dir = './.data/sample_docs'\n",
        "try:\n",
        "  os.makedirs(sample_dir, exist_ok = True)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sample_docs = [\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/1405/1405.4053v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00001v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00003v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00004v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00005v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00006v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00007v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00007v2.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00008v1.pdf',\n",
        "  'gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00009v1.pdf'\n",
        "]\n",
        "sample_doc_names = [(x.split('/')[-1].split('v')[0], os.path.join(sample_dir, x.split('/')[-1])) for x in sample_docs]\n",
        "\n",
        "for doc_url in sample_docs:\n",
        "  !gsutil cp \"$doc_url\" \"$sample_dir\""
      ],
      "metadata": {
        "id": "wHkuWeP_GxUK",
        "outputId": "0985fd63-354f-4fae-f6a4-90b406b3d72c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/1405/1405.4053v1.pdf...\n",
            "/ [0 files][    0.0 B/143.1 KiB]                                                \r/ [1 files][143.1 KiB/143.1 KiB]                                                \r\n",
            "Operation completed over 1 objects/143.1 KiB.                                    \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00001v1.pdf...\n",
            "/ [1 files][ 25.1 MiB/ 25.1 MiB]                                                \n",
            "Operation completed over 1 objects/25.1 MiB.                                     \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00003v1.pdf...\n",
            "/ [1 files][ 94.3 KiB/ 94.3 KiB]                                                \n",
            "Operation completed over 1 objects/94.3 KiB.                                     \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00004v1.pdf...\n",
            "/ [1 files][585.4 KiB/585.4 KiB]                                                \n",
            "Operation completed over 1 objects/585.4 KiB.                                    \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00005v1.pdf...\n",
            "\n",
            "Operation completed over 1 objects/4.5 MiB.                                      \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00006v1.pdf...\n",
            "/ [1 files][ 22.9 MiB/ 22.9 MiB]                                                \n",
            "Operation completed over 1 objects/22.9 MiB.                                     \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00007v1.pdf...\n",
            "/ [1 files][  8.8 MiB/  8.8 MiB]                                                \n",
            "Operation completed over 1 objects/8.8 MiB.                                      \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00007v2.pdf...\n",
            "/ [1 files][  8.8 MiB/  8.8 MiB]                                                \n",
            "Operation completed over 1 objects/8.8 MiB.                                      \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00008v1.pdf...\n",
            "/ [1 files][ 25.8 MiB/ 25.8 MiB]                                                \n",
            "Operation completed over 1 objects/25.8 MiB.                                     \n",
            "Copying gs://arxiv-dataset/arxiv/arxiv/pdf/2309/2309.00009v1.pdf...\n",
            "/ [1 files][369.2 KiB/369.2 KiB]                                                \n",
            "Operation completed over 1 objects/369.2 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "class Publication:\n",
        "  def __init__(self, title, authors, abstract, paragraphs):\n",
        "    self.title = title\n",
        "    self.authors = authors\n",
        "    self.abstract = abstract\n",
        "    self.paragraphs = paragraphs\n",
        "\n",
        "class PublicationMetaLoader:\n",
        "  def __init__(self, paper_id):\n",
        "    self.paper_id = paper_id\n",
        "\n",
        "  def load(self):\n",
        "    (title, authors, abstract) = extract_metadata(self.paper_id)\n",
        "    return Publication(title, authors, abstract, [])\n",
        "\n",
        "class PublicationLoader:\n",
        "  def __init__(self, paper_id, file_path):\n",
        "    self.file_path = file_path\n",
        "    self.paper_id = paper_id\n",
        "\n",
        "  def load(self):\n",
        "    f = open(self.file_path, 'rb')\n",
        "    pdfReader = PyPDF2.PdfReader(f)\n",
        "\n",
        "    fpage_text = self.to_text(pdfReader.pages[0])\n",
        "    pub = PublicationMetaLoader(self.paper_id).load()\n",
        "    pub.paragraphs = [fpage_text]\n",
        "\n",
        "    f.close()\n",
        "    return pub\n",
        "\n",
        "  def to_text(self, page):\n",
        "    return page.extract_text()\n"
      ],
      "metadata": {
        "id": "CPwz2K2eQBAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pubs = []\n",
        "for doc in sample_doc_names:\n",
        "  try:\n",
        "    pubs.append(PublicationMetaLoader(doc[0]).load())\n",
        "  except Exception as inst:\n",
        "    print(inst, doc[0])\n",
        "\n",
        "print(len(pubs))"
      ],
      "metadata": {
        "id": "la7I8bwgQyZX",
        "outputId": "e6631288-08ba-4b73-c721-94eb30d193f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "doc = fitz.open(sample_doc_names[0][1])\n",
        "\n",
        "prev_y0 = 0\n",
        "lines = []\n",
        "line_text = []\n",
        "fonts = []\n",
        "\n",
        "for page in doc:\n",
        "  for block in page.get_text('dict')[\"blocks\"]:\n",
        "    if block['type'] != 0:\n",
        "      continue\n",
        "\n",
        "    for line in block['lines']:\n",
        "      y0 = int(line['bbox'][1])\n",
        "\n",
        "      if y0 != prev_y0:\n",
        "        lines.append(' '.join(line_text))\n",
        "        line_text = []\n",
        "        fonts = []\n",
        "\n",
        "      for s in line['spans']:\n",
        "        fonts.append((s['font'], s['size']))\n",
        "        line_text.extend(s['text'].split(' '))\n",
        "      prev_y0 = y0\n"
      ],
      "metadata": {
        "id": "WkDTPikJeKRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "CLmM-FaUY6Nv",
        "outputId": "c689d494-72ee-4c3b-e038-121129d94e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def author_set(pubs):\n",
        "  temp = set()\n",
        "  for pub in pubs:\n",
        "    temp.update(pub.authors)\n",
        "  return list(temp)\n",
        "\n",
        "def cosine_score(X1, X2):\n",
        "  return cosine(X1, X2)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "def to_embeddings(pubs):\n",
        "  texts = [pub.abstract.lower() for pub in pubs]\n",
        "\n",
        "  #print(sent_tokenize(text))\n",
        "  # words = word_tokenize(text)\n",
        "\n",
        "  # eng_stopwords = stopwords.words(\"english\")\n",
        "  # words = [w for w in words if w not in eng_stopwords]\n",
        "  # words = [w for w in words if re.match('^[a-z_-]+$', w)]\n",
        "  X = vectorizer.fit_transform(texts)\n",
        "  return (X, vectorizer.get_feature_names_out())\n",
        "\n",
        "total_len = len(pubs)\n",
        "train_pubs = pubs[0:int(.8*total_len)]\n",
        "test_pubs = pubs[len(train_pubs):]\n",
        "\n",
        "uniq_authors = author_set(train_pubs)\n",
        "\n",
        "X_train, V = to_embeddings(train_pubs)\n",
        "refernce_vector = np.zeros(X_train.shape[1]) + 0.001\n",
        "\n",
        "print(X_train.shape, len(V), cosine_score(refernce_vector, X_train[0].toarray()[0]))\n",
        "\n",
        "train_df = pd.DataFrame(columns=[\"doc_index\", \"doc_title\", \"sim_score\", \"author\"])\n",
        "for i, pub in enumerate(train_pubs):\n",
        "  for author in pub.authors:\n",
        "    train_df.loc[len(train_df)] = [i, pub.title, cosine_score(refernce_vector, X_train[i].toarray()[0]), author]\n",
        "train_df.sort_values(by=\"sim_score\", inplace=True)\n",
        "\n",
        "X_test = vectorizer.transform([pub.abstract.lower() for pub in test_pubs])\n",
        "\n",
        "print(train_df)\n",
        "\n",
        "for t in X_test:\n",
        "  score = cosine_score(refernce_vector, t.toarray()[0])\n",
        "  print(score, train_df.iloc[(train_df['sim_score']-score).abs().argsort()[:2]])\n"
      ],
      "metadata": {
        "id": "fKP_GbQ-Yjek",
        "outputId": "5294e030-42e9-46fb-cefa-d7480b132981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 421) 421 0.6767738956621577\n",
            "   doc_index                                          doc_title  sim_score  \\\n",
            "6          6  When Measures are Unreliable: Imperceptible Ad...   0.597698   \n",
            "7          7  When Measures are Unreliable: Imperceptible Ad...   0.597698   \n",
            "1          1  QuanAnts Machine: A Quantum Algorithm for Biom...   0.613642   \n",
            "5          5                          Dual Radar SAR Controller   0.639775   \n",
            "4          4  High Spectral Spatial Resolution Synthetic Hyp...   0.667388   \n",
            "0          0  Distributed Representations of Sentences and D...   0.676774   \n",
            "3          3     José Díaz Bejarano (1933-2019). A Bibliography   0.695475   \n",
            "2          2  Laser-assisted inelastic electron scattering b...   0.784275   \n",
            "\n",
            "              author  \n",
            "6         Yuchen Sun  \n",
            "7         Yuchen Sun  \n",
            "1  Phuong-Nam Nguyen  \n",
            "5       Josiah Smith  \n",
            "4          Yajie Sun  \n",
            "0         Quoc V. Le  \n",
            "3       J.M. Vaquero  \n",
            "2     Gabriela Buica  \n",
            "0.8427848456449878    doc_index                                          doc_title  sim_score  \\\n",
            "2          2  Laser-assisted inelastic electron scattering b...   0.784275   \n",
            "3          3     José Díaz Bejarano (1933-2019). A Bibliography   0.695475   \n",
            "\n",
            "           author  \n",
            "2  Gabriela Buica  \n",
            "3    J.M. Vaquero  \n",
            "0.9027851607263364    doc_index                                          doc_title  sim_score  \\\n",
            "2          2  Laser-assisted inelastic electron scattering b...   0.784275   \n",
            "3          3     José Díaz Bejarano (1933-2019). A Bibliography   0.695475   \n",
            "\n",
            "           author  \n",
            "2  Gabriela Buica  \n",
            "3    J.M. Vaquero  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}