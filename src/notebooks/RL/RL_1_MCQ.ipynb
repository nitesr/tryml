{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 6\n",
    "Suppose there are two slot machines that you can play (we will call them “Machine A” and “Machine B”). Machine A will pay you $0.50 each time you play it. Machine B will pay you nothing with probability 9/10 and will pay you $20 with probability 1/10 each time you play it.\n",
    "\n",
    "What is the policy that maximizes the expected total discounted reward, and what is the expected total discounted reward achieved by this policy? Assume a discount factor of 0.9.\n",
    "1. Always Play A \n",
    "2. Always Play B\n",
    "3. Alternate Play A & Play B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999999999999997\n",
      "19.99999999999999\n",
      "13.249999999999995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "P1 = np.array([\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "P2 = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "P3 = np.array([\n",
    "    [1/2, 1/2],\n",
    "    [1/2, 1/2]\n",
    "])\n",
    "reward = np.array([0.5, 1/10*20])\n",
    "\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "def compute_total_discounted_reward(policy, MAX_ITER=100000):\n",
    "    V = np.array([0, 0])\n",
    "    for i in range(0, MAX_ITER):\n",
    "        V = reward + discount_factor*policy.dot(V)\n",
    "    return V\n",
    "\n",
    "print(\n",
    "    compute_total_discounted_reward(P1)[0],\n",
    "    compute_total_discounted_reward(P2)[1],\n",
    "    compute_total_discounted_reward(P3).max(), sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 8\n",
    "Suppose there are two slot machines that you can play (we will call them “Machine A” and “Machine B”). Machine A will pay you $0.50 each time you play it. Machine B will pay you nothing with probability 9/10 and will pay you $20 with probability 1/10 each time you play it.\n",
    "\n",
    "Suppose that you will play the slot machines described in Question 6 repeatedly, but now you do not know the payouts of the machines ahead of time. You will use the following algorithm, which is like a simplified version of Q-learning:\n",
    "\n",
    "Maintain estimates of each machine’s payout, which we will call Q_A and Q_B. We will initially set these estimates to Q_A = Q_B = 0.\n",
    "Play the machine with the greatest estimated payout. That is, if Q_A > Q_B then play Machine A. If Q_B > Q_A then play Machine B. If Q_A = Q_B, pick a machine to play at random.\n",
    "Each time you play Machine i, use the payout p received on that play to update the estimated payout for the machine as Q_i := (1-a)*Q_i + a*p. \n",
    "\n",
    "What are the value states with a epsilon of 0.5 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4998, 2.    ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def policy(V, epsilon=0.5):\n",
    "    if random.random() <= epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    elif V[0] > V[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def reward(slot):\n",
    "    return 0.5 if slot == 0 else 2\n",
    "\n",
    "Q = np.array([0.0, 0.0])\n",
    "alpha = 0.98\n",
    "\n",
    "for i in range(0, 10):\n",
    "    slot = policy(Q)\n",
    "    r = reward(slot)\n",
    "    Q[slot] = (1-alpha)*Q[slot] + alpha*r\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 18\n",
    "\n",
    "state   |   1   |   2   |   3\n",
    "--------|-------|-------|------\n",
    "1       |   1/4 |   1/2 |   1/4\n",
    "2       |   1/3 |   0   |   2/3\n",
    "3       |   1/2 |   0   |   1/2\n",
    "\n",
    "Suppose that we receive a reward of 1 unit every time we visit state 2, and receive zero reward otherwise. If we start from state 1, what is the expected total discounted reward achieved with a discount factor of 0.9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9148936170212754\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = np.array([\n",
    "    [1/4, 1/2, 1/4],\n",
    "    [1/3, 0, 2/3],\n",
    "    [1/2, 0, 1/2]\n",
    "])\n",
    "\n",
    "R = np.array([0, 1, 0])\n",
    "\n",
    "V = np.array([0, 0, 0])\n",
    "discount_factor = 0.9\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    V = R + discount_factor*P.dot(V)\n",
    "    \n",
    "print(V[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12\n",
    "You will roll a six-sided die. If the die lands on one, the game is over. Otherwise you have a choice: You can stop and I will pay you N dollars where N is the number that you rolled, or you can try again. You can retry as many times as you like until you decide to stop or you roll a one. \n",
    "\n",
    "What is the policy that maximizes the expected total discounted reward (assuming a discount factor of 0.9)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[state, action] ->\n",
      " [[ 0.          0.        ]\n",
      " [30.92857143 32.14285714]\n",
      " [31.92857143 32.14285714]\n",
      " [40.         32.14285714]\n",
      " [50.         32.14285714]\n",
      " [60.         32.14285714]]\n",
      "\n",
      "Optimal Policy: We stop if dice value is 4, 5, 6 otherwise roll\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "states = np.array([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "rewards_roll = np.array([0, 0, 0, 0, 0, 0])\n",
    "rewards_stop = np.array([0, 2, 3, 4, 5, 6])\n",
    "\n",
    "ACTION_STOP = 0\n",
    "ACTION_ROLL = 1\n",
    "actions = np.array([ACTION_STOP, ACTION_ROLL])\n",
    "P_stop = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1]\n",
    "])\n",
    "P_roll = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],\n",
    "    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],\n",
    "    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],\n",
    "    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],\n",
    "    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "])\n",
    "discount_factor = 0.9\n",
    "\n",
    "V = np.zeros(len(states))\n",
    "\n",
    "def q_policy(state, Q, epsilon=0.01):\n",
    "    if random.random() <= epsilon:\n",
    "        return random.randint(ACTION_STOP, ACTION_ROLL)\n",
    "    \n",
    "    return ACTION_STOP if Q[state][ACTION_STOP] > Q[state][ACTION_ROLL] else ACTION_ROLL\n",
    "\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "Q[:,ACTION_STOP] =  rewards_stop + P_stop.dot(discount_factor*V)\n",
    "Q[:,ACTION_ROLL] =  rewards_roll + P_roll.dot(discount_factor*V)\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    for state in states:\n",
    "        action = q_policy(state, Q)\n",
    "        P = P_roll[state] if action == ACTION_ROLL else P_stop[state]\n",
    "        r = rewards_roll[state] if action == ACTION_ROLL else rewards_stop[state]\n",
    "        Q[state, action] = Q[state, action] + (r + P.dot(discount_factor*V))\n",
    "    V = np.array([np.max(Q[s]) for s in states])\n",
    "    \n",
    "\n",
    "print('Q[state, action] ->\\n', Q)\n",
    "\n",
    "print('\\nOptimal Policy: We stop if dice value is 4, 5, 6 otherwise roll')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
