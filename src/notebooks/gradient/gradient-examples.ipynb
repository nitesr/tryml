{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8622]) <> 0.8622314929962158\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def x_square(x):\n",
    "    return x**2\n",
    "\n",
    "def d_x_square(x):\n",
    "    return 2*x\n",
    "\n",
    "X = torch.rand(1, requires_grad=True)\n",
    "x = X.item()\n",
    "\n",
    "Y = x_square(X)\n",
    "Y.backward()\n",
    "print(X.grad, \"<>\", d_x_square(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5458]) <> -1.545842170715332\n"
     ]
    }
   ],
   "source": [
    "def lossfn(y_actual, y_pred):\n",
    "    return y_pred - y_actual\n",
    "\n",
    "def d_x_lossfn(y_actual, y_pred, x):\n",
    "    # d(loss)/dx => d(loss)dy * dy/dx\n",
    "    return (0 - 1) * d_x_square(x)\n",
    "\n",
    "y_actual = 10.0\n",
    "X = torch.rand(1, requires_grad=True)\n",
    "x = X.item()\n",
    "Y_pred = x_square(X)\n",
    "y_pred = x_square(x)\n",
    "\n",
    "loss = lossfn(Y_pred, y_actual)\n",
    "loss.backward()\n",
    "print(X.grad, \"<>\", d_x_lossfn(y_actual, y_pred, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-36., dtype=torch.float64) <> -36\n"
     ]
    }
   ],
   "source": [
    "def lossfn2(y_actual, y_pred):\n",
    "    return (y_pred - y_actual) ** 2\n",
    "\n",
    "def d_lossfn2(y_actual, y_pred, x):\n",
    "    # df/dx where f = g**2 and g = y - y_pred\n",
    "    # df/dx = df/dg . dg/dx = 2g . dg/dx\n",
    "    return  2 * lossfn(y_pred, y_actual) * d_x_lossfn(y_actual, y_pred, x)\n",
    "    \n",
    "x = 1\n",
    "y_actual = 10\n",
    "X = torch.tensor(x, dtype=float, requires_grad=True)\n",
    "loss = lossfn2(y_actual, x_square(X))\n",
    "loss.backward()\n",
    "\n",
    "print(X.grad, \"<>\", d_lossfn2(y_actual, x_square(x), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000, dtype=torch.float64) tensor(1., dtype=torch.float64) <> 2.5 1\n"
     ]
    }
   ],
   "source": [
    "def linear_fn(w, x, b):\n",
    "    return w*x+b\n",
    "\n",
    "def d_w_linear_fn(w, x, b):\n",
    "    # d(w.x+b)/dw = x\n",
    "    return x\n",
    "\n",
    "def d_b_linear_fn(w, x, b):\n",
    "    # d(w.x+b)/db = 1\n",
    "    return 1\n",
    "\n",
    "w = 5\n",
    "b = 4\n",
    "x = 2.5\n",
    "W = torch.tensor(w, dtype=float, requires_grad=True)\n",
    "B = torch.tensor(b, dtype=float, requires_grad=True)\n",
    "X = torch.tensor(x, dtype=float, requires_grad=True)\n",
    "Z = linear_fn(W, x, B)\n",
    "Z.backward()\n",
    "print(W.grad, B.grad, \"<>\", d_w_linear_fn(w, x, b), d_b_linear_fn(w, x, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62., dtype=torch.float64, grad_fn=<AddBackward0>) tensor([1., 5., 6.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([ 2., 10., 12.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return sum(x**2);\n",
    "\n",
    "x = np.array([1.0, 5.0, 6.0])\n",
    "X  = torch.tensor(x, requires_grad=True, dtype=float)\n",
    "Y = f(X)\n",
    "Y.backward()\n",
    "print(Y, X)\n",
    "print(X.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear function z = w.x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B= tensor([[1.1714, 0.1635, 0.9987]], requires_grad=True)\n",
      "X= tensor([[ 1.1939,  0.1343],\n",
      "        [-0.1130,  0.0077],\n",
      "        [-0.2277, -1.1534],\n",
      "        [ 1.6545, -0.7241],\n",
      "        [-0.4871,  0.5044]], requires_grad=True)\n",
      "W= tensor([[-0.3280,  1.1977,  0.4110],\n",
      "        [ 1.8343, -0.7891,  0.8515]], requires_grad=True)\n",
      "torch.Size([5, 2]) torch.Size([2, 3])\n",
      "Q= torch.Size([5, 3]) tensor([[-0.1453,  1.3240,  0.6051],\n",
      "        [ 0.0513, -0.1415, -0.0399],\n",
      "        [-2.0409,  0.6374, -1.0757],\n",
      "        [-1.8709,  2.5530,  0.0635],\n",
      "        [ 1.0849, -0.9814,  0.2293]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Z= tensor([[ 1.0261,  1.4875,  1.6037],\n",
      "        [ 1.2227,  0.0220,  0.9588],\n",
      "        [-0.8695,  0.8009, -0.0770],\n",
      "        [-0.6995,  2.7165,  1.0622],\n",
      "        [ 2.2564, -0.8179,  1.2279]], grad_fn=<AddBackward0>)\n",
      "O= tensor(11.9209, grad_fn=<SumBackward0>)\n",
      "dw tensor([[ 2.0206,  2.0206,  2.0206],\n",
      "        [-1.2311, -1.2311, -1.2311]])\n",
      "dx tensor([[1.2807, 1.8967],\n",
      "        [1.2807, 1.8967],\n",
      "        [1.2807, 1.8967],\n",
      "        [1.2807, 1.8967],\n",
      "        [1.2807, 1.8967]]) <> tensor(1.2807, grad_fn=<AddBackward0>) tensor(1.8967, grad_fn=<AddBackward0>)\n",
      "db tensor([[5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "X = torch.randn(5, 2, requires_grad=True)\n",
    "W = torch.randn(2, 3, requires_grad=True)\n",
    "B = torch.randn(1, 3, requires_grad=True)\n",
    "\n",
    "print(\"B=\", B)\n",
    "print(\"X=\", X)\n",
    "print(\"W=\", W)\n",
    "\n",
    "print(X.shape, W.shape)\n",
    "Q = torch.tensordot(X, W, dims=1)\n",
    "Q.retain_grad()\n",
    "print(\"Q=\", Q.shape, Q)\n",
    "\n",
    "Z = Q + B\n",
    "Z.retain_grad()\n",
    "print(\"Z=\", Z)\n",
    "\n",
    "O = torch.sum(Z)\n",
    "print(\"O=\", O)\n",
    "\n",
    "O.backward()\n",
    "#print(\"dz\", Z.grad)\n",
    "print(\"dw\", W.grad)\n",
    "print(\"dx\", X.grad, \"<>\", sum(W[0]), sum(W[1]))\n",
    "print(\"db\", B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive function\n",
    "\n",
    "Computation graph (CS231 course)\n",
    "\n",
    "h(t) = w_h * h(t-1)\n",
    "![comp_graph_recursive_function.jpg](./comp_graph_recursive_function.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) <> 2.0 <> 2.0\n",
      "tensor(4.) <> 4.0 <> 4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "W_h = torch.tensor(2.0, requires_grad=True)\n",
    "w_h = W_h.item()\n",
    "\n",
    "H0 = torch.tensor(0.5, requires_grad=True)\n",
    "h0 = H0.item()\n",
    "\n",
    "H1 = W_h * H0\n",
    "h1 = H1.item()\n",
    "\n",
    "H2 = W_h * H1\n",
    "h2 = H2.item()\n",
    "\n",
    "H2.backward()\n",
    "dw_h_over_h0 = 0 #because H0 is constant for w_h\n",
    "dw_h_over_h1 = h0 + w_h * dw_h_over_h0\n",
    "dw_h_over_h2 = h1 + w_h * dw_h_over_h1\n",
    "\n",
    "dh0_over_h0 = 1\n",
    "dh0_over_h1 = w_h * dh0_over_h0\n",
    "dh0_over_h2 = w_h * dh0_over_h1\n",
    "\n",
    "print(W_h.grad, \"<>\", dw_h_over_h2, \"<>\", (h0 * w_h + h1))\n",
    "print(H0.grad, \"<>\", dh0_over_h2, \"<>\", (w_h * w_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recursive with tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.1888) <> 8.18880672682621 <> 8.18880672682621\n",
      "tensor(0.9093) <> 0.9092516899108887 <> 0.9092516899108887\n"
     ]
    }
   ],
   "source": [
    "# h0 = 0.5\n",
    "# w_h = 2.0\n",
    "# w_y = 40.0\n",
    "# h1_raw = w_h * h0\n",
    "# h1 = tanh(h1_raw)\n",
    "# h2_raw = w_h * h1\n",
    "# h2 = tanh(h2_raw)\n",
    "# y2 = w_y * h2\n",
    "# find dy2/dw_h & dy2/dw_y ?\n",
    "# y2 = 40 * tanh(w_h * tanh(w_h * 0.5))\n",
    "\n",
    "import torch\n",
    "\n",
    "W_h = torch.tensor(2.0, requires_grad=True)\n",
    "w_h = W_h.detach().item()\n",
    "\n",
    "W_y = torch.tensor(40.0, requires_grad=True)\n",
    "w_y = W_y.detach().item()\n",
    "\n",
    "H0 = torch.tensor(0.5, requires_grad=False)\n",
    "h0 = H0.item()\n",
    "\n",
    "H1_raw = W_h * H0\n",
    "h1_raw = H1_raw.item()\n",
    "H1 = torch.tanh(H1_raw)\n",
    "h1 = H1.detach().item()\n",
    "\n",
    "H2_raw = W_h * H1\n",
    "h2_raw = H2_raw.detach().item()\n",
    "H2 = torch.tanh(H2_raw)\n",
    "Y2 = W_y * H2\n",
    "h2 = H2.detach().item()\n",
    "\n",
    "Y2.backward()\n",
    "\n",
    "# analytic method for derivatives\n",
    "#   dy/dw_y = h2 * dw_y/dw_y = h2 (product rule)\n",
    "#   dy/dw_h = h2 * dw_y/dw_h + w_y * dh2/dw_h = w_y * dh2/dw_h (product rule & w_y is contant for w_h)\n",
    "#   dh2/dw_h = 1 - h2_raw**2 * dh2_raw/dw_h (chain rule)\n",
    "#   dh2_raw/dw_h = h1 * dw_h/dw_h + w_h * dh1/dw_h = h1 + w_h * dh1/dw_h (product rule)\n",
    "#   dh1/dw_h = 1 - h1_raw**2 * dh1_raw/dw_h (chain rule)\n",
    "#   dh1_raw/dw_h = h0 * dw_h/dw_h + w_h * dh0/dw_h = h0 (product rule & h0 is constant)\n",
    "dh0_dwh = 0 #because H0 is constant for w_h\n",
    "dh1raw_dwh = h0 + w_h * dh0_dwh\n",
    "dh1_dwh = (1 - h1**2) * dh1raw_dwh\n",
    "dh2raw_dwh = h1 + w_h * dh1_dwh\n",
    "dh2_dwh = (1 - h2**2) * dh2raw_dwh\n",
    "dwy_dwh = 0\n",
    "dy2_dwh = h2 * dwy_dwh + w_y * dh2_dwh\n",
    "\n",
    "dh2_dwy = 0\n",
    "dy2_dwy = h2 + w_y * dh2_dwy\n",
    "\n",
    "# backward pass over computation graph\n",
    "dy2 = 1\n",
    "dw_y = h2 * 1\n",
    "dh2 = w_y * 1\n",
    "dh2_raw = (1 - h2**2) * dh2\n",
    "dh1 = w_h * dh2_raw\n",
    "dh1_raw = (1 - h1**2) * dh1\n",
    "dw_h = h1 * dh2_raw + h0 * dh1_raw\n",
    "\n",
    "print(W_h.grad, \"<>\", dy2_dwh, \"<>\", dw_h)\n",
    "print(W_y.grad, \"<>\", dy2_dwy, \"<>\", dw_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z_x = X . W_x.T + B_x\n",
    "# Z_h = H . W_h.T + B_h\n",
    "# H = tanh(Z_h)\n",
    "# Y = H . W_y.T + B_y\n",
    "# Y_sum += Y\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TorchRNN:\n",
    "    def __init__(self, input_dim=1, hidden_dim=3, output_dim=1, N=2):\n",
    "        self.f_x = torch.nn.Linear(in_features=1, out_features=3)\n",
    "        self.f_h = torch.nn.Linear(in_features=3, out_features=3)\n",
    "        self.f_y = torch.nn.Linear(in_features=3, out_features=1)\n",
    "        \n",
    "        self.H = torch.tensor(np.zeros((1, 3), dtype=np.float32))\n",
    "        self.N = N\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        J_sum = 0\n",
    "        loss = 0\n",
    "        Hs, Ys_hat, diffs, Xs = [self.H], [], [], []\n",
    "        for i in range(self.N):\n",
    "            Z_x = self.f_x(X[i])\n",
    "            Z_h = self.f_h(self.H)\n",
    "            self.H = torch.tanh(Z_x + Z_h)\n",
    "            Y_hat = self.f_y(self.H)\n",
    "            diff = (Y[i] - Y_hat).squeeze(dim=1)\n",
    "            J_sum += diff**2\n",
    "            loss += self.loss_fn(Y_hat.squeeze(dim=1), Y[i])\n",
    "            \n",
    "            Hs.append(self.H)\n",
    "            Ys_hat.append(Y_hat)\n",
    "            diffs.append(diff)\n",
    "            Xs.append(X[i])\n",
    "            \n",
    "        \n",
    "        self.cache = (Hs, Ys_hat, diffs, Xs)\n",
    "        self.J_sum = J_sum\n",
    "        self.loss = loss\n",
    "        return loss, J_sum\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "    \n",
    "    def gradients(self):\n",
    "        return (\n",
    "            self.f_x.weight.grad, \n",
    "            self.f_x.bias.grad,\n",
    "            self.f_h.weight.grad,\n",
    "            self.f_h.bias.grad,\n",
    "            self.f_y.weight.grad,\n",
    "            self.f_y.bias.grad\n",
    "        )\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self, W_x, B_x, W_h, B_h, W_y, B_y, H, N=2):\n",
    "        self.W_x = W_x\n",
    "        self.B_x = B_x\n",
    "        self.W_h = W_h\n",
    "        self.B_h = B_h\n",
    "        self.W_y = W_y\n",
    "        self.B_y = B_y\n",
    "        self.H = H\n",
    "        self.N = N\n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        if X.shape[0] < self.N:\n",
    "            raise AssertionError(\"X.shape[0] is less than {}\".format(self.N))\n",
    "        \n",
    "        J_sum = 0\n",
    "        Hs, Ys_hat, diffs, Xs = [self.H], [], [], []\n",
    "        for i in range(self.N):\n",
    "            Z_x = np.dot(X[i], self.W_x.T) + self.B_x\n",
    "            Z_h = np.dot(self.H, self.W_h.T) + self.B_h\n",
    "            self.H = np.tanh(Z_x + Z_h)\n",
    "            Y_hat = np.dot(self.H, self.W_y.T) + self.B_y\n",
    "            diff = np.squeeze(Y[i] - Y_hat, axis=1)\n",
    "            J_sum += diff**2\n",
    "            \n",
    "            Hs.append(self.H)\n",
    "            Ys_hat.append(Y_hat)\n",
    "            diffs.append(diff)\n",
    "            Xs.append(X[i])\n",
    "        \n",
    "        self.cache = (Hs, Ys_hat, diffs, Xs)\n",
    "        self.J_sum = J_sum\n",
    "        return J_sum\n",
    "\n",
    "    def backward(self):\n",
    "        Hs, Ys_hat, diffs, Xs = self.cache\n",
    "        dW_x, dB_x, dW_h, dB_h, dW_y, dB_y = 0, 0, 0, 0, 0, 0\n",
    "        H_iter = reversed(Hs)\n",
    "        H_cur = next(H_iter)\n",
    "        dH_next = np.zeros(Hs[0].shape)\n",
    "        dJ_sum = 1\n",
    "        for t in reversed(range(self.N)):\n",
    "            H_prev = next(H_iter)\n",
    "            \n",
    "            #diffs[t] = Y[t] - Y_hat[t]\n",
    "            #J[t] = ( diffs[t] ) ** 2\n",
    "            dJ = 2 * diffs[t] * (0 - 1) * dJ_sum\n",
    "            dY = np.full(Ys_hat[0].shape, dJ)\n",
    "        \n",
    "            #Y_hat[t] = H[t] . W_y.T + B_y\n",
    "            dB_y = dB_y + dY\n",
    "            dW_y = dW_y + np.dot(H_cur.T, dY).T\n",
    "            dH = np.dot(dY, self.W_y) + dH_next\n",
    "            \n",
    "            # H[t] = tanh( H_raw[t] )\n",
    "            dH_raw_local = 1 - H_cur * H_cur\n",
    "        \n",
    "            # Z_xs[t] = X[t]. W_x.T + B_x\n",
    "            # Z_hs[t] = H[t-1]. W_h.T + B_h\n",
    "            # H_raw[t] = Z_xs[t] + Z_hs[t] \n",
    "            dH_raw = dH_raw_local * dH\n",
    "            dB_h = dB_h + dH_raw\n",
    "            dW_h = dW_h + np.dot(H_prev.T, dH_raw).T\n",
    "            dW_x = dW_x + np.dot(Xs[t].T, dH_raw).T\n",
    "            \n",
    "            dH_next = np.dot(dH_raw, self.W_h)\n",
    "            H_cur = H_prev\n",
    "        \n",
    "        self.grads = (dW_x, dB_x, dW_h, dB_h, dW_y, dB_y)\n",
    "    \n",
    "    def gradients(self):\n",
    "        return self.grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J_sum: by_torch=147.34588623046875, by_hand=[147.34589557]\n",
      "diffs: by_torch=[tensor([5.6724], grad_fn=<SqueezeBackward1>), tensor([10.7317], grad_fn=<SqueezeBackward1>)], by_hand=[array([5.6724266]), array([10.73170406])]\n",
      "dB_y: by_torch=tensor([-32.8083]) by_hand=[[-32.80826132]]\n",
      "dW_y: by_torch=tensor([[-31.6833, -25.5239,  -5.9449]]) by_hand=[[-31.68331576 -25.52391181  -5.94494814]]\n",
      "dW_H: by_torch=tensor([[0.4772, 0.3028, 0.1222],\n",
      "        [0.9417, 0.5975, 0.2411],\n",
      "        [2.9400, 1.8654, 0.7527]]) by_hand=[[0.47715462 0.30275505 0.12216023]\n",
      " [0.94172597 0.59752601 0.24109892]\n",
      " [2.93996332 1.86540947 0.75268389]]\n",
      "dB_H: by_torch=tensor([1.3071, 1.0650, 3.6685]) by_hand=[[1.30710812 1.06500711 3.66852105]]\n",
      "dW_X: by_torch=tensor([[1.8133],\n",
      "        [2.0641],\n",
      "        [6.7876]]) by_hand=[1.81332578 2.06409273 6.78755478]\n",
      "dB_X: by_torch=tensor([1.3071, 1.0650, 3.6685]) by_hand=0\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "torchRNN = TorchRNN(\n",
    "    input_dim=1, \n",
    "    hidden_dim=3, \n",
    "    output_dim=1, \n",
    "    N=N)\n",
    "\n",
    "vanilaRNN = VanillaRNN(\n",
    "    torchRNN.f_x.weight.detach().numpy(),\n",
    "    torchRNN.f_x.bias.detach().numpy(),\n",
    "    torchRNN.f_h.weight.detach().numpy(),\n",
    "    torchRNN.f_h.bias.detach().numpy(),\n",
    "    torchRNN.f_y.weight.detach().numpy(),\n",
    "    torchRNN.f_y.bias.detach().numpy(),\n",
    "    torchRNN.H.detach().numpy(),\n",
    "    N=N\n",
    ")\n",
    "\n",
    "X = np.array([ [1.0], [2.0], [3.0] ])\n",
    "X_torch = torch.tensor(np.array([ [1.0], [2.0], [3.0] ], dtype=np.float32))\n",
    "\n",
    "Y = np.array([ [5.0], [10.0], [15.0] ])\n",
    "Y_torch = torch.tensor(np.array([ [5.0], [10.0], [15.0] ], dtype=np.float32))\n",
    "\n",
    "#torchRNN.f_h.weight.register_hook(lambda grad: print(\"from hook\", grad))\n",
    "\n",
    "tJ_sum, _ = torchRNN.forward(X_torch, Y_torch)\n",
    "torchRNN.backward()\n",
    "(tdW_x, tdB_x, tdW_h, tdB_h, tdW_y, tdB_y) = torchRNN.gradients()\n",
    "\n",
    "J_sum = vanilaRNN.forward(X, Y)\n",
    "vanilaRNN.backward()\n",
    "(dW_x, dB_x, dW_h, dB_h, dW_y, dB_y) = vanilaRNN.gradients()\n",
    "\n",
    "print(\"J_sum: by_torch={}, by_hand={}\".format(tJ_sum, J_sum))\n",
    "print(\"diffs: by_torch={}, by_hand={}\".format(torchRNN.cache[2], vanilaRNN.cache[2]))\n",
    "\n",
    "print(\"dB_y: by_torch={} by_hand={}\".format(tdB_y, dB_y))\n",
    "print(\"dW_y: by_torch={} by_hand={}\".format(tdW_y, dW_y))\n",
    "\n",
    "print(\"dW_H: by_torch={} by_hand={}\".format(tdW_h, dW_h))\n",
    "print(\"dB_H: by_torch={} by_hand={}\".format(tdB_h, dB_h))\n",
    "\n",
    "print(\"dW_X: by_torch={} by_hand={}\".format(tdW_x, dW_x))\n",
    "print(\"dB_X: by_torch={} by_hand={}\".format(tdB_x, dB_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://arxiv.org/pdf/1802.01528.pdf\n",
    "\n",
    "https://youtu.be/d14TUNcbn1k?si=hyEeGpEt5hP1XVHA\n",
    "\n",
    "https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
