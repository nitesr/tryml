{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to the MNIST data\n",
    "\n",
    "For this classification project, we will be using the MNIST data set that contain 70,000 images of handwritten digits. In this data set, each row represents an image and each column a pixel from a 28 by 28 pixel image. The MNIST dataset is widely used to train classifiers and can be fetched using the helper function sklearn.datasets.fetch_openml.\n",
    "\n",
    "The code below will download the data, and sample 20,000 rows from the original dataset. I will be reducing the number of rows in the dataset to decrease model size and to reduce build time for this project. The code below will also plot the first image in the data set which we can see is the number eight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "\n",
    "# Randomly sample 20000 rows from the original dataset\n",
    "mnist_data = (\n",
    "    mnist\n",
    "    .data\n",
    "    .sample(n=20000, random_state=42, axis=0, replace=False)\n",
    ")\n",
    "\n",
    "# Slice target by the same row sampling\n",
    "target = (\n",
    "    mnist\n",
    "    .target\n",
    "    .loc[mnist_data.index].astype('uint8')\n",
    ")\n",
    "\n",
    "# Reshape values to be 28x28\n",
    "some_digit_image = (\n",
    "    mnist_data\n",
    "    .iloc[0]\n",
    "    .values\n",
    "    .reshape(28,28)\n",
    "    .astype('float32')\n",
    ")\n",
    "plt.imshow(some_digit_image, cmap = \"binary\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 2. Training a K-Nearest Neighbors Classifier\n",
    "\n",
    "First we will split the data into training and test set then train a K-nearest neighbour classifier using thescikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to train KNN Classifier and show scores\n",
    "def train_knn_model(features:np.array, target:np.array):\n",
    "\n",
    "    # Train KNN Classifier\n",
    "    knnclf = KNeighborsClassifier(weights='distance', n_neighbors=4)\n",
    "    knnclf.fit(features, target)\n",
    "    scores = cross_val_score(\n",
    "        knnclf, features, target, scoring='accuracy', cv=10\n",
    "    )\n",
    "    print(f'Cross Validation Scores: {scores}')\n",
    "    print(f'Average accuracy: {np.mean(scores)}')\n",
    "    return knnclf, scores\n",
    "\n",
    "# Split data to training and test set\n",
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "        mnist_data, target, test_size = 0.2, random_state = 42\n",
    ")\n",
    "knnclf, scores = train_knn_model(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves a decent average accuracy of 96% from cross validation. Let's evaluate the model's performance on the test_features data set and plot a confusion matrix with the show_cm function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def show_cm(y_true, y_pred, labels):\n",
    "\n",
    "    # Display Confusion matrix and show accuracy scores\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=labels)\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {score}')\n",
    "    disp.plot();\n",
    "\n",
    "# Make predictions\n",
    "test_target_pred = knnclf.predict(test_features)\n",
    "# Show confusion matrix\n",
    "show_cm(test_target, test_target_pred, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the accuracy on the test data set, we can see that our model fits the data. We get very similar prediction accuracy when comparing accuracies between the training and testing set. \n",
    "\n",
    "Furthermore, a confusion matrix, like above, is very effective in helping visualise the gaps in the model's performance. It will help us understand the kind of errors that the classifier is making.\n",
    "\n",
    "The matrix indicates that there were 16 instances where the number 4 was misidentified for the number 9, and 12 instances where the number 8 was misidentified for the number 5. \n",
    "\n",
    "Looking at the images below, it is possible to see why some of these errors may occur as the number 4 and 9 do share some similar features. Likewise for the number 8 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_digits(pixel_vals):\n",
    "\n",
    "    some_digit_image = (\n",
    "        pixel_vals\n",
    "        .values\n",
    "        .reshape(28,28)\n",
    "        .astype('float32')\n",
    "    )\n",
    "    plt.imshow(some_digit_image, cmap = \"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "fours = train_features[train_target == 4]\n",
    "nines = train_features[train_target == 9]\n",
    "eights = train_features[train_target == 8]\n",
    "fives = train_features[train_target == 5]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); show_digits(fours.iloc[0])\n",
    "plt.subplot(222); show_digits(nines.iloc[0])\n",
    "plt.subplot(223); show_digits(eights.iloc[1])\n",
    "plt.subplot(224); show_digits(fives.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This insight is not going to affect model deployment on AWS but will help guide strategies to further improve the model. \n",
    "For now, we will save the model locally to be containerised as part of the lambda function using Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(knnclf, 'app/knnclf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 3. Initialising AWS S3 Bucket\n",
    "\n",
    "The image below illustrates the overall resource infrastructure that will need to be deployed to support our lambda function. There are three key resources requirements for our application:\n",
    "\n",
    "1. S3 Bucket to store data.\n",
    "2. API gateway to manage HTTP requests.\n",
    "3. Lambda function containing the predictive logic.\n",
    "\n",
    "![](overview.png)\n",
    "\n",
    "#### Serverless deployment of ML models\n",
    "\n",
    "1. Test data is uploaded to a S3 bucket.\n",
    "\n",
    "2. To initiate the lambda function, a POST HTTP request is sent through the Amazon API Gateway. \n",
    "\n",
    "3. Initialisation of the lambda function executes code that downloads the data from the S3 bucket and performs predictions. \n",
    "\n",
    "4. A HTTP response is returned to client with the predictions as a data payload.\n",
    "\n",
    "The Lambda function will contain Python code that performs a prediction based on the test_features dataset stored on a S3 bucket. \n",
    "\n",
    "Therefore, we will first need to initialise a S3 bucket where we can host our data.\n",
    "\n",
    "To do so, we will be interacting with AWS using the AWS Python SDK `boto3`. This package contains all the dependencies we require to integrate Python projects with AWS. \n",
    "\n",
    "Let's initialise a S3 bucket with the code below.\n",
    "\n",
    "***Note:*** *The `bucket_name` has to be unique therefore you will have to replace the bucket_name with a name that is not taken.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<YOUR_KEY>\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<YOUR_SECRET>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def create_bucket(bucket_name:str) -> dict:\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    # The 'LocationConstraint' is unnecesary here since we want default region of 'us-east-1'\n",
    "    # uncommenting this will throw an error\n",
    "    # DO NOT create resources in other regions as you will not have permissions to delete them later\n",
    "    response = s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        # CreateBucketConfiguration={\n",
    "        #     'LocationConstraint': 'us-east-1'\n",
    "        # }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# This name must be unique!\n",
    "bucket_name = 'nm-lambda-demo-2024'\n",
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Test data to S3\n",
    "\n",
    "The S3 bucket will host our test_features data set which we can call in our lambda function to perform a prediction.\n",
    "\n",
    "To save an object currently in our workspace, we will be making use of BytesIO function from the io library. This will enable us to temporary store the test_features data set in a file object. This file object can be uploaded onto a S3 bucket by calling the .upload_fileobj function.\n",
    "\n",
    "The bucket variable defines the destination S3 bucket and the key variable will define the file path in the bucket. The `bucket` and `key` variables will form part of the data payload in the POST HTTP request to our lambda function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import joblib\n",
    "import boto3\n",
    "\n",
    "def UploadToS3(data, bucket:str, key:str):\n",
    "\n",
    "    with BytesIO() as f:\n",
    "        joblib.dump(data, f)\n",
    "        f.seek(0)\n",
    "        (\n",
    "            boto3\n",
    "            .client('s3')\n",
    "            .upload_fileobj(Bucket=bucket, Key=key, Fileobj=f)\n",
    "        )\n",
    "\n",
    "key =  'validation/test_features.joblib'\n",
    "UploadToS3(test_features, bucket_name, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all objects in S3 bucket\n",
    "\n",
    "We can check if the objects have been uploaded with the helper function below. list_s3_objects will list all objects in the defined bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def listS3Objects(bucket:str) -> list:\n",
    "\n",
    "     # Connect to s3 resource\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "\n",
    "    # List all object keys in s3 bucket\n",
    "    obj_list = [object_summary.key for object_summary in my_bucket.objects.all()]\n",
    "    return obj_list\n",
    "\n",
    "listS3Objects(bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully initialised a S3 bucket to store the test_feature data. The next two key resources, API Gateway and lambda function, will be deployed using AWS Serverless Application Model (SAM). Refer to main blog for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 4. Deploying and Testing AWS Lambda Functions with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Building and testing the application locally.\n",
    "\n",
    "AWS SAM provide functionality to build and locally test applications before deployment. \n",
    "Ensure docker is running. \n",
    "\n",
    "In a terminal window, navigate to the project directory and build the application in SAM.\n",
    "\n",
    "a) If using Apple M Silicon, in `template_no_auth.yaml` line 25 change the value from `x86_64` to `arm64`\n",
    "\n",
    "b) Ensure docker is running. In a terminal window, navigate to the project directory and build the application in SAM. Note, if having trouble with poetry lock file or install within the `Dockerfile`, re-download the `poetry.lock` file from [here](https://raw.githubusercontent.com/lloydhamilton/aws_lambda_no_authoriser/master/poetry.lock)\n",
    "\n",
    "`sam build -t template_no_auth.yaml`\n",
    "\n",
    "c) Locally deploy the dockerised lambda function.\n",
    "\n",
    "`sam local start-api`\n",
    "\n",
    "d) Locally invoke the function at http://127.0.0.1:3000/predict. Your URL may differ.\n",
    "\n",
    "***Note***: *The `bucket` and `key` variable which references the test_feature data set on S3 will need to be passed as part of the data payload in the POST HTTP request.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knnclf = joblib.load('app/knnclf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'bucket':bucket_name,\n",
    "    'key':key,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Content-type': \"application/json\"\n",
    "}\n",
    "\n",
    "# Main code for post HTTP request\n",
    "url = \"http://127.0.0.1:3000/predict\"\n",
    "# If you see the error, ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n",
    "# That means you need to `aws configure` your AWS key/secret\n",
    "response = requests.request(\"POST\", url, headers=headers, data=json.dumps(data))\n",
    "lambda_predictions = np.array(response.json())\n",
    "show_cm(test_target, lambda_predictions, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Deploying on AWS Lambda\n",
    "\n",
    "As easy as it was to deploy locally, SAM will also handle all the heavy lifting to deploy on AWS Lambda.\n",
    "\n",
    "a) Build the application in SAM.\n",
    "\n",
    "`sam build -t template_no_auth.yaml`\n",
    "\n",
    "b) Deploy the application.\n",
    "\n",
    "`sam deploy --guided`\n",
    "\n",
    "Follow the prompts that guides you through the deployment configurations. Most of the settings I used were the default value with a few exceptions. See main blog for details.\n",
    "\n",
    "c) Invoke your function by replacing the URL in the code below with the URL from the output out of a successfully deployed stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_digits(pixel_vals):\n",
    "\n",
    "    some_digit_image = (\n",
    "        pixel_vals\n",
    "        .values\n",
    "        .reshape(28,28)\n",
    "        .astype('float32')\n",
    "    )\n",
    "    plt.imshow(some_digit_image, cmap = \"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "fours = train_features[train_target == 4]\n",
    "nines = train_features[train_target == 9]\n",
    "eights = train_features[train_target == 8]\n",
    "fives = train_features[train_target == 5]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); show_digits(fours.iloc[0])\n",
    "plt.subplot(222); show_digits(nines.iloc[0])\n",
    "plt.subplot(223); show_digits(eights.iloc[1])\n",
    "plt.subplot(224); show_digits(fives.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'bucket':bucket_name,\n",
    "    'key':key,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Content-type': \"application/json\"\n",
    "}\n",
    "\n",
    "# Main code for post HTTP request\n",
    "# Update `url` with the value given after deployment\n",
    "# NOTE: The first request may time out (due to long model loading?), but retry may be successful\n",
    "url = \"https://gvpqb9emm7.execute-api.us-east-1.amazonaws.com/dev/predict\"\n",
    "response = requests.request(\"POST\", url, headers=headers, json=data)\n",
    "lambda_predictions = np.array(response.json())\n",
    "show_cm(test_target, lambda_predictions, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert deploying_lambda.ipynb --to html --template classic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_serverless_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
