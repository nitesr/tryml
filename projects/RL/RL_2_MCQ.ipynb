{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL - 2 : Question 2-4, 8, 9\n",
    "\n",
    "1 <-> 2 <-> 3 <-> 4 -> Goal\n",
    "\n",
    "let’s assume the agent starts in each numbered cell with equal probability. It also moves left or right with equal probability. The agent will receive -1 reward at each numbered cell (for lollygagging), and 1 reward at the goal cell.\n",
    "\n",
    "What is the expected undiscounted return the agent receives?\n",
    "\n",
    "**Question 3:**\n",
    "how does the expected return change when we have the agent move right with probability 0.75 and 0.25, respectively?\n",
    "\n",
    "**Question 4:**\n",
    "What is the expected undiscounted return of an agent following the optimal policy?\n",
    "\n",
    "**Question 8:**\n",
    "What are the advantages of moving right from cell 1, 2, 3, and 4?\n",
    "\n",
    "**Question 9:**\n",
    "What is the difference in state values V-optimal(1) - V(1) for cell 1 under the two policies ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected return (1/2, 1/2): -10.499999999999995\n",
      "Expected return (1/4, 3/4): -2.462962962962962\n",
      "Expected return (3/4, 1/4): -99.4899706960878\n",
      "Expected return (optimal): -0.5\n",
      "advantages of moving right from cell 1, 2, 3, and 4: [0. 2. 4. 6.]\n",
      "difference in state values V-optimal(1) - V(1): 11.999999999999993\n"
     ]
    }
   ],
   "source": [
    "# Expected return is sum (P(s) * V(s)) s -> 1, 2, 3, 4, G\n",
    "#  1/4 * [ V(1) + V(2) + V(3) + V(4) ]\n",
    "#  V(s) = sum-over-actions { P(a | s) * Q(s, a) }\n",
    "#  Q(s, a) = sum-over-states { P(s' | s, a) * [ r(s, a) + gamma * V(s') ] }\n",
    "#\n",
    "#  V(G) = 1 # no transitions from the goal state.\n",
    "#\n",
    "#  V(4) = 1/2 * Q(4, <-) + 1/2 * Q(4, ->)\n",
    "#  Q(4, ->) = 1 * [1 + 1 * V(G) ]\n",
    "#  Q(4, <-) = 1 * [ -1 + 1 * V(3) ]\n",
    "#  V(4) =  1/2 * 2 + 1/2 * [-1 + V(3)] = 1/2 + 1/2 * V(3)\n",
    "# etc..\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_optimal_return(\n",
    "    P_left, P_right,\n",
    "    rewards=np.array([-1, -1, -1, -1, 1]), discount_factor=1):\n",
    "    \n",
    "    Q = np.zeros((5, 2))\n",
    "    V = np.zeros(5)\n",
    "\n",
    "    for i in range(0, 1000):\n",
    "        #left \n",
    "        for s in range(1, 4):\n",
    "            Q[s][0] = 1 * ( rewards[s-1] + discount_factor * V[s-1])\n",
    "        Q[0][0] = 0\n",
    "        \n",
    "        #right \n",
    "        for s in range(0, 4):\n",
    "            Q[s][1] = 1 * ( rewards[s+1] + discount_factor * V[s+1])\n",
    "        \n",
    "        for s in range(1, 4):\n",
    "            V[s] = max(Q[s][0], Q[s][1])\n",
    "        V[0] = Q[0][1]\n",
    "\n",
    "    return (1/4 * np.sum(V[0:4])), V, Q\n",
    "\n",
    "def compute_expected_return(\n",
    "    P_left, P_right,\n",
    "    rewards=np.array([-1, -1, -1, -1, 1]), discount_factor=1):\n",
    "    \n",
    "    Q = np.zeros((5, 2))\n",
    "    V = np.zeros(5)\n",
    "\n",
    "    for i in range(0, 1000):\n",
    "        #left \n",
    "        for s in range(1, 4):\n",
    "            Q[s][0] = 1 * ( rewards[s-1] + discount_factor * V[s-1])\n",
    "        \n",
    "        #right \n",
    "        for s in range(0, 4):\n",
    "            Q[s][1] = 1 * ( rewards[s+1] + discount_factor * V[s+1])\n",
    "        \n",
    "        for s in range(0, 4):\n",
    "            V[s] = P_left[s] * Q[s][0] + P_right[s] * Q[s][1]\n",
    "\n",
    "    return (1/4 * np.sum(V[0:4])), V, Q\n",
    "\n",
    "P_left = np.array([0, 1/2, 1/2, 1/2, 0])\n",
    "P_right = np.array([1, 1/2, 1/2, 1/2, 0])\n",
    "print(\"Expected return (1/2, 1/2):\", compute_expected_return(P_left, P_right)[0])\n",
    "\n",
    "P_left = np.array([0, 1/4, 1/4, 1/4, 0])\n",
    "P_right = np.array([1, 3/4, 3/4, 3/4, 0])\n",
    "print(\"Expected return (1/4, 3/4):\", compute_expected_return(P_left, P_right)[0])\n",
    "\n",
    "P_left = np.array([0, 3/4, 3/4, 3/4, 0])\n",
    "P_right = np.array([1, 1/4, 1/4, 1/4, 0])\n",
    "print(\"Expected return (3/4, 1/4):\", compute_expected_return(P_left, P_right)[0])\n",
    "\n",
    "P_left = np.array([0, 0, 0, 0, 0])\n",
    "P_right = np.array([1, 1, 1, 1, 0])\n",
    "print(\"Expected return (optimal):\", compute_expected_return(P_left, P_right)[0])\n",
    "\n",
    "\n",
    "P_left = np.array([0, 1/2, 1/2, 1/2, 0])\n",
    "P_right = np.array([1, 1/2, 1/2, 1/2, 0])\n",
    "_, V, Q = compute_expected_return(P_left, P_right)\n",
    "print(\"advantages of moving right from cell 1, 2, 3, and 4:\", Q[0:4, 1] - V[0:4])\n",
    "_, V_opt, _ = compute_optimal_return(P_left, P_right)\n",
    "print(\"difference in state values V-optimal(1) - V(1):\", V_opt[0]-V[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL - 2 : Question 7\n",
    "\n",
    "1 <-> 2 <-> 3 <-> 4 -> Goal\n",
    "\n",
    "let’s assume the agent starts in each numbered cell with equal probability. It also moves left or right with equal probability. The agent will receive -1 reward at each numbered cell (for lollygagging), and 1 reward at the goal cell.\n",
    "\n",
    "We initialize the agent to a uniform policy, using weights and bias. w=[6, 0, 0, 0] & b=0 giving a uniform policy. The agent starts in cell 4, taking quite a detour 4 -> 3 -> 2 -> 1 -> 2 -> 3 -> 4 -> Goal. \n",
    "\n",
    "With this episode, let’s update the policy with REINFORCE. How would the policy weights change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_MCQ.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_MCQ.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_MCQ.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/n0c09jf/code/github/tryml/notebooks/RL/RL_2_MCQ.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/__init__.py:457\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(textwrap\u001b[39m.\u001b[39mdedent(\u001b[39m'''\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[39m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[39m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[39m            \u001b[39m\u001b[39m'''\u001b[39m)\u001b[39m.\u001b[39mstrip()) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[39mraise\u001b[39;00m  \u001b[39m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(_C):\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m name[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mBase\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    459\u001b[0m         __all__\u001b[39m.\u001b[39mappend(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.optim import SGD\n",
    "\n",
    "states = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "L = nn.Linear(in_features=4, out_features=1, bias=False)\n",
    "with torch.no_grad():\n",
    "    L.weight.copy_(torch.tensor([6.0, 0.0, 0.0, 0.0], dtype=torch.float32))\n",
    "\n",
    "model = nn.Sequential(L, nn.Sigmoid())\n",
    "trajectory = [(3, 0, -1), (2, 0, -1), (1, 0, -1), (0, 1, -1), (1, 1, -1), (2, 1, -1), (3, 1, 1)]\n",
    "\n",
    "input = []\n",
    "actions = []\n",
    "returns = []\n",
    "\n",
    "for i in range(0, len(trajectory)):\n",
    "    (s, a, r) = trajectory[i]\n",
    "    input.append(states[s])\n",
    "    actions.append(a)\n",
    "\n",
    "\n",
    "returns = [trajectory[-1][2]]\n",
    "for i in reversed(range(0, len(trajectory)-1)):\n",
    "    returns.append(trajectory[i][2]+returns[-1])\n",
    "returns.reverse()\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=1)\n",
    "ip = torch.tensor(np.array(input), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "probs = model.forward(ip)\n",
    "dist = Bernoulli(probs=probs)\n",
    "loss = -1 * (dist.log_prob(torch.tensor(actions, dtype=torch.float32))*torch.as_tensor(returns)).mean()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(model[0].weight)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
